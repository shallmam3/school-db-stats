import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re

# --- æ ¸å¿ƒé€»è¾‘ ---

def google_search_url(school_name, api_key):
    """
    ä½¿ç”¨ Serper API (Google) ç»•è¿‡äº‘ç«¯ IP é™åˆ¶ï¼Œç²¾å‡†å¯»æ‰¾ç›®æ ‡ç½‘å€
    """
    url = "https://google.serper.dev/search"
    
    # ç»„åˆæ›´ç²¾å‡†çš„æœç´¢è¯ï¼Œæé«˜å‘½ä¸­ç‡
    query = f"{school_name} å›¾ä¹¦é¦† æ•°æ®åº“å¯¼èˆª åˆ—è¡¨"
    
    payload = str({
        "q": query,
        "gl": "cn",
        "hl": "zh-cn"
    }).replace("'", '"')
    
    headers = {
        'X-API-KEY': api_key,
        'Content-Type': 'application/json'
    }

    try:
        response = requests.post(url, headers=headers, json={"q": query, "gl": "cn", "hl": "zh-cn"})
        results = response.json()
        
        # è·å–è‡ªç„¶æœç´¢ç»“æœçš„ç¬¬ä¸€æ¡
        if 'organic' in results and len(results['organic']) > 0:
            top_link = results['organic'][0]['link']
            print(f"API æ‰¾åˆ°é“¾æ¥: {top_link}")
            return top_link
        else:
            return None
    except Exception as e:
        st.error(f"API è¿æ¥å¤±è´¥: {e}")
        return None

def is_chinese(string):
    """åˆ¤æ–­æ˜¯å¦åŒ…å«ä¸­æ–‡"""
    for char in string:
        if '\u4e00' <= char <= '\u9fa5':
            return True
    return False

def analyze_page(url):
    """æŠ“å–å¹¶åˆ†æé¡µé¢"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, 'html.parser')

        # æ¸…ç†å¹²æ‰°é¡¹
        for tag in soup(['header', 'footer', 'nav', 'script', 'style', 'noscript']):
            tag.decompose()
        
        links = soup.find_all('a')
        db_list = []
        
        for link in links:
            text = link.get_text(strip=True)
            # æ™ºèƒ½è¿‡æ»¤ï¼šå»æ‰ç”±äºâ€œé¦–é¡µâ€ã€â€œç™»å½•â€ç­‰çŸ­è¯ï¼Œä»¥åŠè¿‡é•¿çš„å¥å­
            if 3 < len(text) < 50: 
                db_list.append(text)
        
        # å»é‡
        db_list = list(set(db_list))
        
        cn_dbs = [db for db in db_list if is_chinese(db)]
        other_dbs = [db for db in db_list if not is_chinese(db)]
                
        return cn_dbs, other_dbs

    except Exception as e:
        st.error(f"æ— æ³•è¯»å–è¯¥å­¦æ ¡é¡µé¢: {e}")
        return None, None

# --- UI ç•Œé¢ ---

st.set_page_config(page_title="é«˜æ ¡æ•°æ®åº“æ™ºèƒ½ç»Ÿè®¡", page_icon="ğŸ•µï¸")

st.title("ğŸ•µï¸ é«˜æ ¡æ•°æ®åº“å…¨è‡ªåŠ¨ç»Ÿè®¡")
st.markdown("é›†æˆ **Google Search API**ï¼Œè‡ªåŠ¨çªç ´åçˆ¬è™«é™åˆ¶ï¼Œå¯»æ‰¾æ•°æ®åº“åˆ—è¡¨ã€‚")

# ä¾§è¾¹æ è¾“å…¥ Keyï¼Œé¿å…æ¯æ¬¡éƒ½è¦è¾“
with st.sidebar:
    st.header("âš™ï¸ è®¾ç½®")
    api_key = st.text_input("2ba768b7f52d792da0b87486b73acfc5d305f4a3", type="password", help="å» serper.dev å…è´¹ç”³è¯·")
    st.markdown("[ğŸ‘‰ ç‚¹å‡»è·å–å…è´¹ Key](https://serper.dev/)")

school_input = st.text_input("è¯·è¾“å…¥å­¦æ ¡å…¨ç§°ï¼ˆä¾‹å¦‚ï¼šé™•è¥¿å¸ˆèŒƒå¤§å­¦ï¼‰")

if st.button("å¼€å§‹å…¨è‡ªåŠ¨åˆ†æ"):
    if not api_key:
        st.error("è¯·å…ˆåœ¨å·¦ä¾§ä¾§è¾¹æ å¡«å…¥ API Keyï¼")
    elif not school_input:
        st.warning("è¯·å…ˆè¾“å…¥å­¦æ ¡åç§°")
    else:
        # 1. è°ƒç”¨ API è‡ªåŠ¨æœç´¢
        with st.status("ğŸ¤– æ­£åœ¨æŒ‡æŒ¥ Google æœç´¢æ•°æ®åº“ç½‘å€...", expanded=True) as status:
            target_url = google_search_url(school_input, api_key)
            
            if target_url:
                status.write(f"âœ… æˆåŠŸæ‰¾åˆ°åœ°å€: {target_url}")
                status.write("â¬‡ï¸ æ­£åœ¨æ½œå…¥é¡µé¢æŠ“å–æ•°æ®...")
                
                # 2. åˆ†æé¡µé¢
                cn_list, en_list = analyze_page(target_url)
                
                if cn_list is not None:
                    status.update(label="åˆ†æå®Œæˆï¼", state="complete", expanded=False)
                    
                    # 3. å±•ç¤ºç»“æœ
                    st.divider()
                    st.success(f"ğŸ“Š {school_input} åˆ†ææŠ¥å‘Š")
                    
                    col1, col2, col3 = st.columns(3)
                    col1.metric("ä¸­æ–‡æ•°æ®åº“", f"{len(cn_list)}")
                    col2.metric("å¤–æ–‡/å…¶ä»–", f"{len(en_list)}")
                    col3.metric("æ€»è®¡", f"{len(cn_list) + len(en_list)}")
                    
                    tab1, tab2 = st.tabs(["ğŸ“ ä¸­æ–‡åº“æ¸…å•", "ğŸŒ å¤–æ–‡åº“æ¸…å•"])
                    with tab1:
                        st.dataframe(pd.DataFrame(cn_list, columns=["æ•°æ®åº“åç§°"]), use_container_width=True)
                    with tab2:
                        st.dataframe(pd.DataFrame(en_list, columns=["æ•°æ®åº“åç§°"]), use_container_width=True)
                else:
                    status.update(label="æŠ“å–é¡µé¢å¤±è´¥", state="error")
            else:
                status.update(label="æœç´¢æœªæ‰¾åˆ°æœ‰æ•ˆç»“æœ", state="error")
                st.error("API è¿”å›ç©ºç»“æœï¼Œå¯èƒ½è¯¥å­¦æ ¡æ²¡æœ‰å…¬å¼€çš„æ•°æ®åº“åˆ—è¡¨é¡µã€‚")