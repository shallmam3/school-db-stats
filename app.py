import streamlit as st
import subprocess
import os
import json
import requests
import time
from bs4 import BeautifulSoup
import pandas as pd
from collections import Counter

# --- 1. ç¯å¢ƒè‡ªæ£€ä¸åˆå§‹åŒ– (Playwright) ---
# ç¡®ä¿äº‘ç«¯ç¯å¢ƒå®‰è£…äº†æµè§ˆå™¨å†…æ ¸
if "playwright_installed" not in st.session_state:
    if not os.path.exists(os.path.expanduser("~/.cache/ms-playwright")):
        with st.spinner("æ­£åœ¨åˆå§‹åŒ–äº‘ç«¯æµè§ˆå™¨ç»„ä»¶... (é¦–æ¬¡è¿è¡Œçº¦éœ€1åˆ†é’Ÿ)"):
            subprocess.run(["playwright", "install", "chromium"])
    st.session_state.playwright_installed = True

try:
    from playwright.sync_api import sync_playwright
except ImportError:
    st.error("ä¸¥é‡é”™è¯¯ï¼šæœªæ‰¾åˆ° playwright åº“ã€‚è¯·æ£€æŸ¥ requirements.txt")
    st.stop()

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

def get_api_key():
    """ä»åå°è¯»å– API Key"""
    try:
        return st.secrets["SERPER_API_KEY"]
    except Exception:
        return None

def google_search_url(school_name, api_key):
    """åˆ©ç”¨ Google æœç´¢æ‰¾åˆ°ç›®æ ‡ç½‘å€"""
    url = "https://google.serper.dev/search"
    queries = [
        f"{school_name} å›¾ä¹¦é¦† æ•°æ®åº“ åˆ—è¡¨",
        f"{school_name} å›¾ä¹¦é¦† ç”µå­èµ„æº å¯¼èˆª",
        f"{school_name} library database list"
    ]
    
    headers = {
        'X-API-KEY': api_key,
        'Content-Type': 'application/json; charset=utf-8'
    }

    for query in queries:
        try:
            payload = json.dumps({
                "q": query, 
                "gl": "cn", 
                "hl": "zh-cn"
            }, ensure_ascii=False).encode('utf-8')

            response = requests.post(url, headers=headers, data=payload, timeout=10)
            if response.status_code == 200:
                results = response.json()
                if 'organic' in results and len(results['organic']) > 0:
                    return results['organic'][0]['link']
        except Exception:
            continue
    return None

def get_dynamic_page_content(url):
    """åˆ©ç”¨ Playwright æ¸²æŸ“åŠ¨æ€ç½‘é¡µ"""
    with sync_playwright() as p:
        try:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )
            page = context.new_page()
            
            # è®¿é—®é¡µé¢ï¼Œæœ€é•¿ç­‰å¾…60ç§’
            page.goto(url, timeout=60000, wait_until="domcontentloaded")
            
            # å¼ºåˆ¶ç­‰å¾…ï¼Œç¡®ä¿åŠ¨æ€å†…å®¹åŠ è½½å®Œæ¯•
            time.sleep(5)
            
            content = page.content()
            browser.close()
            return content
        except Exception as e:
            print(f"æŠ“å–å¤±è´¥: {e}")
            return None

def is_chinese(string):
    for char in string:
        if '\u4e00' <= char <= '\u9fa5':
            return True
    return False

def analyze_html(html_content):
    """
    ã€æ ¸å¿ƒå‡çº§ã€‘æ™ºèƒ½ç»“æ„åŒ–è§£æ + é»‘åå•è¿‡æ»¤
    """
    if not html_content:
        return [], []
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # 1. æš´åŠ›æ¸…ç†ï¼šåˆ æ‰è‚¯å®šä¸æ˜¯æ•°æ®åº“åˆ—è¡¨çš„åŒºåŸŸ
    for tag in soup(['header', 'footer', 'nav', 'script', 'style', 'noscript', 'iframe', 'form', 'img']):
        tag.decompose()
        
    # 2. æ™ºèƒ½å®šä½ï¼šå¯»æ‰¾â€œå«é“¾æ¥å¯†åº¦æœ€é«˜â€çš„å®¹å™¨ (æ’é™¤ä¾§è¾¹æ )
    all_links = soup.find_all('a')
    parents = []
    
    for link in all_links:
        text = link.get_text(strip=True)
        # åªæœ‰é•¿åº¦åƒæ•°æ®åº“åçš„é“¾æ¥æ‰å‚ä¸æŠ•ç¥¨
        if 2 < len(text) < 60:
            # å¯»æ‰¾å®ƒçš„çˆ¶çº§å®¹å™¨
            parent = link.find_parent(['ul', 'div', 'table', 'tbody', 'section'])
            if parent:
                parents.append(parent)

    # é»˜è®¤å…¨é¡µæœç´¢
    target_area = soup
    
    # å¦‚æœæ‰¾åˆ°äº†èšé›†åŒºï¼Œä¸”é“¾æ¥æ•°é‡è¶…è¿‡5ä¸ªï¼Œå°±é”å®šè¿™ä¸ªåŒºåŸŸ
    if parents:
        top_parent, count = Counter(parents).most_common(1)[0]
        if count > 5:
            target_area = top_parent

    # 3. æå–ä¸é»‘åå•è¿‡æ»¤
    links = target_area.find_all('a')
    
    # é»‘åå•ï¼šå‡¡æ˜¯åŒ…å«è¿™äº›è¯çš„ï¼Œå¤§æ¦‚ç‡æ˜¯å¯¼èˆªæˆ–åŠŸèƒ½æŒ‰é’®
    blacklist = [
        "é¦–é¡µ", "ä¸»é¡µ", "æ¦‚å†µ", "ç®€ä»‹", "é¡»çŸ¥", "æŒ‡å—", "è§„ç« ", "åˆ¶åº¦", 
        "ç™»å½•", "æ³¨å†Œ", "å¯†ç ", "è´¦æˆ·", "è®¤è¯", "é€€å‡º", "æ³¨é”€", "å…¥å£",
        "æ–°é—»", "å…¬å‘Š", "åŠ¨æ€", "é€šçŸ¥", "å…¬ç¤º", "æ‹›è˜", "æ‹›æ ‡", 
        "åŸ¹è®­", "è®²åº§", "è¯¾ç¨‹", "å…šå»º", "æ”¯éƒ¨", "å­¦ä¹ ", "æ•™è‚²", "è§†é¢‘", 
        "è”ç³»", "ç”µè¯", "é‚®ç®±", "åœ°å€", "ç•™è¨€", "åé¦ˆ", "å¸®åŠ©", "é—®ç­”",
        "å¯¼èˆª", "é“¾æ¥", "åœ°å›¾", "English", "æ—§ç‰ˆ", "APP", "å¾®ä¿¡", "å¾®åš",
        "ä¸‹è½½", "å®‰è£…", "é˜…è¯»å™¨", "VPN", "æ ¡å¤–", "è®¿é—®", "æäº¤", "æ›´å¤š",
        "ç‚¹å‡»", "æŸ¥çœ‹", "è¿”å›", "ä¸Šä¸€é¡µ", "ä¸‹ä¸€é¡µ", "è¯•ç”¨", "æ¨è", "ç½®é¡¶", "æ¥æº"
    ]
    
    db_list = []
    for link in links:
        text = link.get_text(strip=True)
        
        # é•¿åº¦è¿‡æ»¤
        if 2 < len(text) < 60:
            # é»‘åå•æ£€æŸ¥
            is_junk = False
            for junk in blacklist:
                if junk in text:
                    is_junk = True
                    break
            
            # å¿…é¡»ä¸æ˜¯åƒåœ¾è¯ï¼Œä¸”ä¸å…¨æ˜¯æ•°å­—(é¡µç )
            if not is_junk and not text.isdigit():
                db_list.append(text)
    
    db_list = list(set(db_list))
    cn_dbs = [db for db in db_list if is_chinese(db)]
    other_dbs = [db for db in db_list if not is_chinese(db)]
    
    return cn_dbs, other_dbs

# --- 3. UI ç•Œé¢ ---

st.set_page_config(page_title="é«˜æ ¡æ•°æ®åº“è‡ªåŠ¨ç»Ÿè®¡", page_icon="ğŸ«", layout="centered")

st.title("ğŸ« é«˜æ ¡æ•°æ®åº“å…¨è‡ªåŠ¨ç»Ÿè®¡")
st.caption("é›†æˆ Serper è‡ªåŠ¨æœç´¢ + Playwright åŠ¨æ€æŠ“å– + æ™ºèƒ½é™å™ª")

# è·å– Key
api_key = get_api_key()

# è¾“å…¥åŒº
col1, col2 = st.columns([3, 1])
with col1:
    school_input = st.text_input("è¯·è¾“å…¥å­¦æ ¡å…¨ç§°", placeholder="ä¾‹å¦‚ï¼šè¥¿å®‰ç§‘æŠ€å¤§å­¦", label_visibility="collapsed")
with col2:
    start_btn = st.button("å¼€å§‹åˆ†æ", type="primary", use_container_width=True)

# çŠ¶æ€åŒº
status_box = st.status("ç­‰å¾…æŒ‡ä»¤...", expanded=False)

if start_btn:
    if not school_input:
        st.toast("è¯·è¾“å…¥æ ¡åï¼")
    elif not api_key:
        st.error("âŒ æœªæ£€æµ‹åˆ° API Keyã€‚è¯·åœ¨ Streamlit Cloud åå° Settings -> Secrets ä¸­é…ç½® SERPER_API_KEYã€‚")
    else:
        # ç¬¬ä¸€æ­¥ï¼šæœç´¢
        status_box.update(label=f"ğŸ” æ­£åœ¨å…¨ç½‘æœç´¢ã€{school_input}ã€‘çš„æ•°æ®åº“åˆ—è¡¨...", state="running", expanded=True)
        target_url = google_search_url(school_input, api_key)
        
        if target_url:
            status_box.write(f"âœ… æ‰¾åˆ°ç›®æ ‡åœ°å€: {target_url}")
            
            # ç¬¬äºŒæ­¥ï¼šæŠ“å–
            status_box.update(label="ğŸš€ æ­£åœ¨å¯åŠ¨äº‘ç«¯æµè§ˆå™¨åŠ è½½é¡µé¢ (çº¦10-20ç§’)...", state="running")
            html_content = get_dynamic_page_content(target_url)
            
            if html_content:
                # ç¬¬ä¸‰æ­¥ï¼šåˆ†æ
                status_box.write("âœ… é¡µé¢åŠ è½½æˆåŠŸï¼Œæ­£åœ¨è¿›è¡Œæ™ºèƒ½è§£æä¸è¿‡æ»¤...")
                cn_list, en_list = analyze_html(html_content)
                status_box.update(label="åˆ†æå®Œæˆï¼", state="complete", expanded=False)
                
                # ç¬¬å››æ­¥ï¼šå±•ç¤º
                total = len(cn_list) + len(en_list)
                
                st.divider()
                st.markdown(f"### ğŸ“Š {school_input}")
                st.caption(f"æ•°æ®æ¥æº: {target_url}")
                
                if total == 0:
                    st.warning("âš ï¸ æœªè¯†åˆ«åˆ°æœ‰æ•ˆæ•°æ®åº“ã€‚å¯èƒ½æ˜¯é¡µé¢ç»“æ„ç‰¹æ®Šæˆ–éœ€è¦æ ¡å†…ç½‘ç™»å½•ã€‚")
                    st.markdown(f"[ç‚¹å‡»æ‰‹åŠ¨è®¿é—®é¡µé¢æ£€æŸ¥]({target_url})")
                else:
                    m1, m2, m3 = st.columns(3)
                    m1.metric("æ€»è®¡", total)
                    m2.metric("ä¸­æ–‡æ•°æ®åº“", len(cn_list))
                    m3.metric("å¤–æ–‡æ•°æ®åº“", len(en_list))
                    
                    with st.expander("ğŸ“„ æŸ¥çœ‹è¯¦ç»†æ¸…å• (å·²è¿‡æ»¤å¯¼èˆªå™ªéŸ³)", expanded=True):
                        tab1, tab2 = st.tabs(["ä¸­æ–‡åº“", "å¤–æ–‡åº“"])
                        with tab1:
                            st.dataframe(pd.DataFrame(cn_list, columns=["åç§°"]), use_container_width=True, hide_index=True)
                        with tab2:
                            st.dataframe(pd.DataFrame(en_list, columns=["åç§°"]), use_container_width=True, hide_index=True)
            else:
                status_box.update(label="âŒ æµè§ˆå™¨åŠ è½½é¡µé¢å¤±è´¥", state="error")
                st.error("ç½‘é¡µåŠ è½½è¶…æ—¶ï¼Œæ— æ³•è·å–å†…å®¹ã€‚")
        else:
            status_box.update(label="âŒ è‡ªåŠ¨æœç´¢å¤±è´¥", state="error")
            st.warning("Google æœªèƒ½æ‰¾åˆ°è¯¥å­¦æ ¡æ˜ç¡®çš„æ•°æ®åº“åˆ—è¡¨é¡µé¢ã€‚")
            manual_url = st.text_input("è¯·æ‰‹åŠ¨ç²˜è´´ç½‘å€å°è¯•ï¼š")