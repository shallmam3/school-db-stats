import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import json

# --- æ ¸å¿ƒé…ç½® ---
st.set_page_config(
    page_title="é«˜æ ¡æ•°æ®åº“ç»Ÿè®¡", 
    page_icon="ðŸ“±",
    layout="centered" # æ‰‹æœºç«¯ä½¿ç”¨ centered å¸ƒå±€è§†è§‰æ›´èšç„¦
)

# --- æ ¸å¿ƒé€»è¾‘ ---

def get_api_key():
    """å®‰å…¨åœ°ä»Ž Streamlit Secrets èŽ·å– Key"""
    try:
        return st.secrets["SERPER_API_KEY"]
    except FileNotFoundError:
        st.error("âŒ æœªé…ç½® API Keyï¼è¯·åœ¨ Streamlit Cloud åŽå° Settings -> Secrets ä¸­æ·»åŠ  SERPER_API_KEYã€‚")
        return None

def google_search_url(school_name, api_key):
    """æœç´¢é€»è¾‘"""
    url = "https://google.serper.dev/search"
    queries = [
        f"{school_name} å›¾ä¹¦é¦† æ•°æ®åº“ åˆ—è¡¨",
        f"{school_name} å›¾ä¹¦é¦† ç”µå­èµ„æº å¯¼èˆª",
    ]
    
    headers = {
        'X-API-KEY': api_key,
        'Content-Type': 'application/json; charset=utf-8'
    }

    for query in queries:
        try:
            payload = json.dumps({
                "q": query, 
                "gl": "cn", 
                "hl": "zh-cn"
            }, ensure_ascii=False).encode('utf-8')

            response = requests.post(url, headers=headers, data=payload, timeout=10)
            if response.status_code == 200:
                results = response.json()
                if 'organic' in results and len(results['organic']) > 0:
                    return results['organic'][0]['link']
        except Exception:
            continue
    return None

def is_chinese(string):
    for char in string:
        if '\u4e00' <= char <= '\u9fa5':
            return True
    return False

def analyze_page(url):
    headers = {'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1'}
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, 'html.parser')
        
        for tag in soup(['header', 'footer', 'nav', 'script', 'style', 'noscript']):
            tag.decompose()
        
        links = soup.find_all('a')
        db_list = []
        for link in links:
            text = link.get_text(strip=True)
            if 3 < len(text) < 50: 
                db_list.append(text)
        
        db_list = list(set(db_list))
        cn_dbs = [db for db in db_list if is_chinese(db)]
        other_dbs = [db for db in db_list if not is_chinese(db)]
        return cn_dbs, other_dbs
    except Exception as e:
        return None, None

# --- æ‰‹æœºç«¯ UI ä¼˜åŒ– ---

st.markdown("### ðŸ« é«˜æ ¡æ•°æ®åº“ç»Ÿè®¡")
st.caption("è‡ªåŠ¨æœç´¢å¹¶ç»Ÿè®¡å›¾ä¹¦é¦†è´­ä¹°çš„æ•°æ®åº“æ•°é‡")

# 1. ç®€æ´çš„è¾“å…¥åŒº
col1, col2 = st.columns([3, 1])
with col1:
    school_input = st.text_input("è¾“å…¥æ ¡å", placeholder="ä¾‹å¦‚ï¼šé™•è¥¿å¸ˆèŒƒå¤§å­¦", label_visibility="collapsed")
with col2:
    start_btn = st.button("å¼€å§‹", type="primary", use_container_width=True)

# 2. çŠ¶æ€æ˜¾ç¤ºåŒºï¼ˆç”¨è¾ƒå°çš„å­—ä½“ï¼‰
status_container = st.empty()

if start_btn:
    api_key = get_api_key()
    
    if not school_input:
        st.toast("âš ï¸ è¯·è¾“å…¥å­¦æ ¡åç§°")
    elif api_key:
        
        # æ­¥éª¤ A: æœç´¢
        status_container.info("ðŸ” æ­£åœ¨å¯»æ‰¾æ•°æ®åº“ç½‘é¡µ...")
        target_url = google_search_url(school_input, api_key)
        
        if target_url:
            # æ­¥éª¤ B: åˆ†æž
            status_container.success(f"âœ… æ‰¾åˆ°ç½‘é¡µï¼Œæ­£åœ¨åˆ†æž...")
            cn_list, en_list = analyze_page(target_url)
            
            status_container.empty() # æ¸…ç©ºçŠ¶æ€æ ï¼Œå±•ç¤ºç»“æžœ
            
            if cn_list is not None:
                total = len(cn_list) + len(en_list)
                
                # --- æ ¸å¿ƒç»“æžœåŒº (å¤§å­—å·å¡ç‰‡) ---
                st.divider()
                st.markdown(f"**{school_input}**")
                
                # ä½¿ç”¨åŽŸç”Ÿ metricï¼Œæ‰‹æœºä¼šè‡ªåŠ¨å †å 
                m1, m2, m3 = st.columns(3)
                m1.metric("æ€»è®¡", total)
                m2.metric("ä¸­æ–‡", len(cn_list))
                m3.metric("å¤–æ–‡", len(en_list))
                
                st.divider()
                
                # --- è¯¦æƒ…åŒº (é»˜è®¤æŠ˜å ï¼ŒèŠ‚çœæ‰‹æœºç©ºé—´) ---
                with st.expander("ðŸ“„ æŸ¥çœ‹è¯¦ç»†åå• (ç‚¹å‡»å±•å¼€)"):
                    st.markdown("**ðŸ‡¨ðŸ‡³ ä¸­æ–‡æ•°æ®åº“**")
                    st.dataframe(pd.DataFrame(cn_list, columns=["åç§°"]), hide_index=True, use_container_width=True)
                    
                    st.markdown("**ðŸŒ å¤–æ–‡/å…¶ä»–æ•°æ®åº“**")
                    st.dataframe(pd.DataFrame(en_list, columns=["åç§°"]), hide_index=True, use_container_width=True)
                    
                st.caption(f"æ•°æ®æ¥æº: {target_url}")
                
            else:
                st.error("æ— æ³•è¯»å–é¡µé¢ï¼Œå¯èƒ½æœ‰é˜²ç«å¢™æ‹¦æˆªã€‚")
        else:
            status_container.warning("æœªæ‰¾åˆ°è¯¥å­¦æ ¡çš„å…¬å¼€æ•°æ®åº“åˆ—è¡¨ã€‚")
            # å…œåº•ï¼šå…è®¸æ‰‹åŠ¨è¾“å…¥
            manual_url = st.text_input("å°è¯•æ‰‹åŠ¨ç²˜è´´ç½‘å€ï¼š")