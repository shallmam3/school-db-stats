import streamlit as st
import subprocess
import os
import json
import requests
import time
from bs4 import BeautifulSoup
import pandas as pd

# --- 1. ç¯å¢ƒåˆå§‹åŒ– ---
if "playwright_installed" not in st.session_state:
    if not os.path.exists(os.path.expanduser("~/.cache/ms-playwright")):
        with st.spinner("æ­£åœ¨åˆå§‹åŒ–äº‘ç«¯æµè§ˆå™¨..."):
            subprocess.run(["playwright", "install", "chromium"])
    st.session_state.playwright_installed = True

try:
    from playwright.sync_api import sync_playwright
except ImportError:
    st.error("è¯·æ£€æŸ¥ requirements.txt æ˜¯å¦åŒ…å« playwright")
    st.stop()

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

def get_api_key():
    try:
        return st.secrets["SERPER_API_KEY"]
    except:
        return None

def google_search_lib_url(school_name, api_key):
    """
    ç¬¬ä¸€æ­¥ï¼šåªæ‰¾å›¾ä¹¦é¦†çš„å…¥å£ï¼Œä¸éœ€è¦ç›´æ¥æ‰¾åˆ°åˆ—è¡¨é¡µ
    è®© Playwright å»åšå…·ä½“çš„ç‚¹å‡»å·¥ä½œ
    """
    url = "https://google.serper.dev/search"
    # æœç´¢ç­–ç•¥ï¼šä¼˜å…ˆæ‰¾å›¾ä¹¦é¦†å®˜ç½‘ï¼Œæˆ–è€…ç›´æ¥æ‰¾æ•°æ®åº“é¡µ
    queries = [
        f"{school_name} å›¾ä¹¦é¦† å®˜ç½‘",
        f"{school_name} å›¾ä¹¦é¦† æ•°æ®åº“",
    ]
    
    headers = {'X-API-KEY': api_key, 'Content-Type': 'application/json; charset=utf-8'}

    for query in queries:
        try:
            payload = json.dumps({"q": query, "gl": "cn", "hl": "zh-cn"}, ensure_ascii=False).encode('utf-8')
            response = requests.post(url, headers=headers, data=payload, timeout=10)
            if response.status_code == 200:
                results = response.json()
                if 'organic' in results and len(results['organic']) > 0:
                    return results['organic'][0]['link']
        except:
            continue
    return None

def extract_from_table(soup):
    """è¡¨æ ¼æå–é€»è¾‘ (ä¿æŒä¸å˜ï¼Œå› ä¸ºè¿™æ˜¯å¯¹çš„)"""
    db_list = []
    tables = soup.find_all('table')
    
    for table in tables:
        text_content = table.get_text()
        keywords = ["æ•°æ®åº“", "èµ„æº", "é¢˜å", "å·²è´­", "è®¢è´­", "ä¸­æ–‡", "å¤–æ–‡"]
        # å¦‚æœè¡¨æ ¼é‡Œæ²¡æœ‰è¿™äº›å…³é”®è¯ï¼Œå°±è·³è¿‡
        if sum(1 for k in keywords if k in text_content) < 2:
            continue
            
        rows = table.find_all('tr')
        for row in rows:
            cells = row.find_all(['td', 'th'])
            for cell in cells:
                links = cell.find_all('a')
                for link in links:
                    text = link.get_text(strip=True)
                    if 2 < len(text) < 60:
                        db_list.append(text)
                if not links:
                     text = cell.get_text(strip=True)
                     if 2 < len(text) < 60 and not text.isdigit():
                         db_list.append(text)
    return db_list

def automated_browser_workflow(start_url):
    """
    ã€çœŸæ­£çš„è‡ªåŠ¨åŒ–æ ¸å¿ƒã€‘
    1. è¿›å…¥é¡µé¢
    2. å¦‚æœå½“å‰é¡µé¢æ²¡æœ‰è¡¨æ ¼ï¼Œè‡ªåŠ¨å¯»æ‰¾â€œå·²è´­èµ„æºâ€ã€â€œç”µå­èµ„æºâ€æŒ‰é’®å¹¶ç‚¹å‡»
    3. æå–æ•°æ®
    """
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Chrome/120.0.0.0 Safari/537.36')
        page = context.new_page()
        
        print(f"æ­£åœ¨è®¿é—®: {start_url}")
        try:
            page.goto(start_url, timeout=30000, wait_until="domcontentloaded")
            time.sleep(2) # ç­‰å¾…æ¸²æŸ“
            
            # --- æ™ºèƒ½è·³è½¬é€»è¾‘ ---
            # æ£€æŸ¥å½“å‰é¡µé¢æœ‰æ²¡æœ‰ç›®æ ‡è¡¨æ ¼
            initial_content = page.content()
            initial_soup = BeautifulSoup(initial_content, 'html.parser')
            initial_data = extract_from_table(initial_soup)
            
            # å¦‚æœå½“å‰é¡µé¢ç›´æ¥å°±æœ‰æ•°æ®ï¼Œå¤ªå¥½äº†ï¼Œç›´æ¥è¿”å›
            if len(initial_data) > 10:
                print("ç›´æ¥åœ¨ç€é™†é¡µæ‰¾åˆ°æ•°æ®")
                browser.close()
                return initial_data

            # å¦‚æœæ²¡æœ‰ï¼Œå¼€å§‹å¯»æ‰¾å…¥å£é“¾æ¥å¹¶ç‚¹å‡»
            print("å½“å‰é¡µé¢æœªå‘ç°è¡¨æ ¼ï¼Œå°è¯•ç‚¹å‡»å¯¼èˆª...")
            
            # å¸¸è§çš„å…¥å£å…³é”®è¯ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
            keywords = ["å·²è´­èµ„æº", "ä¸­æ–‡æ•°æ®åº“", "ç”µå­èµ„æº", "æ•°æ®åº“å¯¼èˆª", "æ‰€æœ‰æ•°æ®åº“", "è®¢è´­èµ„æº"]
            
            found_click = False
            for kw in keywords:
                try:
                    # å¯»æ‰¾åŒ…å«å…³é”®è¯çš„é“¾æ¥
                    # ä½¿ç”¨ Playwright çš„å®šä½å™¨ï¼Œæ¨¡ç³ŠåŒ¹é…æ–‡æœ¬
                    link = page.get_by_text(kw, exact=False).first
                    if link.is_visible():
                        print(f"--> ç‚¹å‡»è·³è½¬: {kw}")
                        link.click(timeout=3000)
                        page.wait_for_load_state("domcontentloaded", timeout=10000)
                        time.sleep(3) # ç­‰å¾…è·³è½¬åçš„è¡¨æ ¼æ¸²æŸ“
                        found_click = True
                        break # è·³è¿‡ä¸€æ¬¡åï¼Œé€šå¸¸å°±æ˜¯ç›®æ ‡é¡µäº†
                except Exception as e:
                    continue # æ²¡æ‰¾åˆ°è¿™ä¸ªè¯ï¼Œæ‰¾ä¸‹ä¸€ä¸ª
            
            if not found_click:
                print("æœªæ‰¾åˆ°æ˜æ˜¾çš„è·³è½¬é“¾æ¥ï¼Œå°è¯•åœ¨å½“å‰é¡µç¡¬è§£æ")

            # --- æœ€ç»ˆæå– ---
            final_content = page.content()
            browser.close()
            
            final_soup = BeautifulSoup(final_content, 'html.parser')
            final_data = extract_from_table(final_soup)
            
            # å…œåº•ï¼šå¦‚æœè¡¨æ ¼æå–è¿˜æ˜¯ç©ºçš„ï¼Œè¯•ç€ç”¨åˆ—è¡¨æ–¹å¼æå–
            if len(final_data) < 5:
                # æ¸…ç†å¹²æ‰°é¡¹
                for tag in final_soup(['header', 'footer', 'nav', 'script', 'style']):
                    tag.decompose()
                clean_links = []
                for link in final_soup.find_all('a'):
                    txt = link.get_text(strip=True)
                    if 4 < len(txt) < 50:
                        clean_links.append(txt)
                return clean_links

            return final_data

        except Exception as e:
            print(f"Browser Error: {e}")
            browser.close()
            return []

def is_chinese(string):
    for char in string:
        if '\u4e00' <= char <= '\u9fa5': return True
    return False

def clean_data(raw_list):
    blacklist = [
        "é¦–é¡µ", "ç™»å½•", "æ³¨å†Œ", "æ›´å¤š", "æŸ¥çœ‹", "è®¢è´­", "è¯•ç”¨", "ç®€ä»‹", "æŒ‡å—", 
        "è¯¦ç»†", "è®¿é—®", "æ ¡å¤–", "å’¨è¯¢", "åé¦ˆ", "ç‚¹å‡»", "ä¸‹è½½", "English",
        "åºå·", "çŠ¶æ€", "ç±»å‹", "åç§°", "æ•°æ®åº“åç§°", "æ“ä½œ", "æ¥æº", "é“¾æ¥", 
        "æäº¤", "éƒ¨é—¨", "ç‰ˆæƒ", "æ‰€æœ‰", "å¯¼èˆª", "æœåŠ¡", "æ¦‚å†µ"
    ]
    clean_list = []
    for item in raw_list:
        text = item.strip()
        if 2 < len(text) < 60 and not text.isdigit():
            if not any(junk in text for junk in blacklist):
                clean_list.append(text)
    return list(set(clean_list))

# --- 3. UI ç•Œé¢ (å›å½’ç®€æ´) ---
st.set_page_config(page_title="é«˜æ ¡æ•°æ®åº“è‡ªåŠ¨ç»Ÿè®¡", page_icon="ğŸ«", layout="wide")

st.title("ğŸ« é«˜æ ¡æ•°æ®åº“å…¨è‡ªåŠ¨ç»Ÿè®¡")
st.caption("è¾“å…¥æ ¡å -> è‡ªåŠ¨è¿›å…¥å›¾ä¹¦é¦† -> è‡ªåŠ¨å¯»æ‰¾å·²è´­èµ„æº -> è¾“å‡ºç»Ÿè®¡")

# ç®€å•çš„è¾“å…¥åŒº
col1, col2 = st.columns([3, 1])
with col1:
    school_input = st.text_input("è¯·è¾“å…¥å­¦æ ¡å…¨ç§°", placeholder="ä¾‹å¦‚ï¼šè¥¿å®‰ç§‘æŠ€å¤§å­¦", label_visibility="collapsed")
with col2:
    start_btn = st.button("å¼€å§‹è‡ªåŠ¨åŒ–åˆ†æ", type="primary", use_container_width=True)

api_key = get_api_key()

if start_btn:
    if not api_key:
        st.error("è¯·åœ¨åå°é…ç½® SERPER_API_KEY")
        st.stop()
    
    if not school_input:
        st.warning("è¯·è¾“å…¥å­¦æ ¡åç§°")
        st.stop()

    status = st.status("ğŸš€ æ­£åœ¨å¯åŠ¨è‡ªåŠ¨åŒ–ç¨‹åº...", expanded=True)
    
    # 1. æœç´¢å…¥å£
    status.write(f"ğŸ” æ­£åœ¨æœç´¢ {school_input} å›¾ä¹¦é¦†å®˜ç½‘...")
    start_url = google_search_lib_url(school_input, api_key)
    
    if start_url:
        status.write(f"ğŸŒ æ‰¾åˆ°å…¥å£: {start_url}")
        status.write("ğŸ¤– æ­£åœ¨æ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®ï¼Œå¯»æ‰¾â€œå·²è´­èµ„æºâ€è¡¨æ ¼...")
        
        # 2. è‡ªåŠ¨åŒ–æµè§ˆ + æ™ºèƒ½è·³è½¬
        raw_dbs = automated_browser_workflow(start_url)
        
        # 3. æ•°æ®æ¸…æ´—
        final_dbs = clean_data(raw_dbs)
        cn_dbs = sorted([d for d in final_dbs if is_chinese(d)])
        en_dbs = sorted([d for d in final_dbs if not is_chinese(d)])
        total = len(cn_dbs) + len(en_dbs)
        
        status.update(label="âœ… å®Œæˆï¼", state="complete", expanded=False)
        
        # --- ç»“æœå±•ç¤º ---
        st.divider()
        st.markdown(f"### ğŸ“Š {school_input} ç»Ÿè®¡ç»“æœ")
        st.caption(f"æ•°æ®æœ€ç»ˆæ¥æºé¡µ: {start_url}") # è¿™é‡Œçš„URLå¯èƒ½æ˜¯è·³è½¬å‰çš„ï¼Œä¸»è¦ä½œå‚è€ƒ
        
        if total == 0:
            st.error("âš ï¸ æœªæå–åˆ°æœ‰æ•ˆæ•°æ®ã€‚")
            st.info("å¯èƒ½æœ‰ä»¥ä¸‹åŸå› ï¼š\n1. è¯¥å­¦æ ¡å®˜ç½‘å¿…é¡»æ ¡å†…VPNæ‰èƒ½è®¿é—®ã€‚\n2. ç½‘é¡µç»“æ„æå…¶ç‰¹æ®Šï¼Œè‡ªåŠ¨åŒ–ç‚¹å‡»æœªå‘½ä¸­ã€‚")
        else:
            # ç»Ÿè®¡å¡ç‰‡
            c1, c2, c3 = st.columns(3)
            c1.metric("ğŸ“š æ€»è®¡èµ„æº", total)
            c2.metric("ğŸ‡¨ğŸ‡³ ä¸­æ–‡æ•°æ®åº“", len(cn_dbs))
            c3.metric("ğŸŒ å¤–æ–‡æ•°æ®åº“", len(en_dbs))
            
            st.divider()
            
            # åŒæ åˆ—è¡¨
            c_left, c_right = st.columns(2)
            with c_left:
                st.subheader(f"ä¸­æ–‡æ•°æ®åº“ ({len(cn_dbs)})")
                if cn_dbs:
                    df = pd.DataFrame(cn_dbs, columns=["åç§°"])
                    df.index += 1
                    st.dataframe(df, use_container_width=True)
            
            with c_right:
                st.subheader(f"å¤–æ–‡æ•°æ®åº“ ({len(en_dbs)})")
                if en_dbs:
                    df = pd.DataFrame(en_dbs, columns=["åç§°"])
                    df.index += 1
                    st.dataframe(df, use_container_width=True)
            
    else:
        status.update(label="âŒ æœç´¢å¤±è´¥", state="error")
        st.error("æ— æ³•æ‰¾åˆ°è¯¥å­¦æ ¡å›¾ä¹¦é¦†å®˜ç½‘ï¼Œè¯·æ£€æŸ¥æ ¡åæ˜¯å¦æ­£ç¡®ã€‚")