import streamlit as st
import subprocess
import os
import json
import requests
import time
from bs4 import BeautifulSoup
import pandas as pd
from collections import Counter

# --- 1. ç¯å¢ƒåˆå§‹åŒ– ---
if "playwright_installed" not in st.session_state:
    if not os.path.exists(os.path.expanduser("~/.cache/ms-playwright")):
        with st.spinner("æ­£åœ¨åˆå§‹åŒ–äº‘ç«¯æµè§ˆå™¨..."):
            subprocess.run(["playwright", "install", "chromium"])
    st.session_state.playwright_installed = True

try:
    from playwright.sync_api import sync_playwright
except ImportError:
    st.error("è¯·æ£€æŸ¥ requirements.txt æ˜¯å¦åŒ…å« playwright")
    st.stop()

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

def get_api_key():
    try:
        return st.secrets["SERPER_API_KEY"]
    except:
        return None

def google_search_url(school_name, api_key):
    """æœç´¢å…¥å£ URL"""
    url = "https://google.serper.dev/search"
    queries = [
        f"{school_name} å›¾ä¹¦é¦† \"å·²è´­èµ„æº\" åˆ—è¡¨",
        f"{school_name} å›¾ä¹¦é¦† æ•°æ®åº“å¯¼èˆª",
        f"{school_name} å›¾ä¹¦é¦† ç”µå­èµ„æºåˆ—è¡¨ site:edu.cn"
    ]
    
    headers = {'X-API-KEY': api_key, 'Content-Type': 'application/json; charset=utf-8'}

    for query in queries:
        try:
            payload = json.dumps({"q": query, "gl": "cn", "hl": "zh-cn"}, ensure_ascii=False).encode('utf-8')
            response = requests.post(url, headers=headers, data=payload, timeout=10)
            if response.status_code == 200:
                results = response.json()
                if 'organic' in results and len(results['organic']) > 0:
                    return results['organic'][0]['link']
        except:
            continue
    return None

def extract_from_table(soup):
    """è¯†åˆ«è¡¨æ ¼å†…å®¹ (å…¼å®¹è¥¿ç§‘å¤§å·¦å³åˆ†æ )"""
    db_list = []
    tables = soup.find_all('table')
    
    for table in tables:
        text_content = table.get_text()
        keywords = ["æ•°æ®åº“", "èµ„æº", "é¢˜å", "å·²è´­", "è®¢è´­", "ä¸­æ–‡", "å¤–æ–‡"]
        if sum(1 for k in keywords if k in text_content) < 2:
            continue
            
        rows = table.find_all('tr')
        for row in rows:
            cells = row.find_all(['td', 'th'])
            for cell in cells:
                links = cell.find_all('a')
                for link in links:
                    text = link.get_text(strip=True)
                    if 2 < len(text) < 60:
                        db_list.append(text)
                if not links:
                     text = cell.get_text(strip=True)
                     if 2 < len(text) < 60 and not text.isdigit():
                         db_list.append(text)
    return db_list

def smart_crawl_and_extract(url):
    """Playwright åŠ¨æ€æŠ“å–"""
    with sync_playwright() as p:
        try:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Chrome/120.0.0.0 Safari/537.36')
            page = context.new_page()
            
            page.goto(url, timeout=60000, wait_until="domcontentloaded")
            time.sleep(3) 
            
            # å°è¯•æ™ºèƒ½ç‚¹å‡»â€œå·²è´­èµ„æºâ€
            content = page.content()
            if "å·²è´­èµ„æº" in content and "ä¸­æ–‡æ•°æ®åº“" not in content:
                try:
                    page.get_by_text("å·²è´­èµ„æº", exact=False).first.click(timeout=3000)
                    time.sleep(3)
                except:
                    pass

            final_content = page.content()
            browser.close()
            
            soup = BeautifulSoup(final_content, 'html.parser')
            db_list = extract_from_table(soup)
            
            # å…œåº•ç­–ç•¥
            if len(db_list) < 5:
                for tag in soup(['header', 'footer', 'nav', 'script', 'style']):
                    tag.decompose()
                for link in soup.find_all('a'):
                    txt = link.get_text(strip=True)
                    if 3 < len(txt) < 50:
                        db_list.append(txt)

            return db_list
        except Exception as e:
            print(f"Error: {e}")
            return []

def is_chinese(string):
    for char in string:
        if '\u4e00' <= char <= '\u9fa5': return True
    return False

def clean_data(raw_list):
    blacklist = [
        "é¦–é¡µ", "ç™»å½•", "æ³¨å†Œ", "æ›´å¤š", "æŸ¥çœ‹", "è®¢è´­", "è¯•ç”¨", "ç®€ä»‹", "æŒ‡å—", 
        "è¯¦ç»†", "è®¿é—®", "æ ¡å¤–", "å’¨è¯¢", "åé¦ˆ", "ç‚¹å‡»", "ä¸‹è½½", "English",
        "åºå·", "çŠ¶æ€", "ç±»å‹", "åç§°", "æ•°æ®åº“åç§°", "æ“ä½œ", "æ¥æº", "é“¾æ¥", 
        "æäº¤", "éƒ¨é—¨", "ç‰ˆæƒ", "æ‰€æœ‰", "å¯¼èˆª"
    ]
    clean_list = []
    for item in raw_list:
        text = item.strip()
        if 2 < len(text) < 60 and not text.isdigit():
            if not any(junk in text for junk in blacklist):
                clean_list.append(text)
    return list(set(clean_list))

# --- 3. UI ç•Œé¢ ---
st.set_page_config(page_title="é«˜æ ¡æ•°æ®åº“ç»Ÿè®¡Pro", page_icon="ğŸ«", layout="wide")

st.title("ğŸ« é«˜æ ¡æ•°æ®åº“å…¨è‡ªåŠ¨ç»Ÿè®¡ (Proç‰ˆ)")

with st.sidebar:
    st.header("âš™ï¸ é…ç½®å‚æ•°")
    api_key = st.text_input("SERPER_API_KEY", value=get_api_key() or "", type="password")
    st.divider()
    school_input = st.text_input("ğŸ« å­¦æ ¡å…¨ç§°", placeholder="ä¾‹å¦‚ï¼šè¥¿å®‰ç§‘æŠ€å¤§å­¦")
    st.markdown("**æˆ–è€…**")
    manual_url = st.text_input("ğŸ”— æŒ‡å®šç›®æ ‡ URL (ç²¾å‡†æ¨¡å¼)", placeholder="ä¾‹å¦‚ï¼š...wbtreeid=6533")
    st.caption("æç¤ºï¼šå¦‚æœè‡ªåŠ¨æœç´¢ä¸å‡†ï¼Œè¯·ç›´æ¥åœ¨æ­¤ç²˜è´´ç›®æ ‡ç½‘å€ã€‚")
    start_btn = st.button("å¼€å§‹åˆ†æ", type="primary", use_container_width=True)

if start_btn:
    if not api_key:
        st.error("è¯·é…ç½® SERPER_API_KEY")
        st.stop()
    
    target_url = None
    status = st.status("æ­£åœ¨åˆå§‹åŒ–...", expanded=True)
    
    if manual_url:
        target_url = manual_url
        status.write(f"ğŸ”— ä½¿ç”¨ç”¨æˆ·æŒ‡å®š URL: {target_url}")
    elif school_input:
        status.write(f"ğŸ” æ­£åœ¨æœç´¢ {school_input}...")
        target_url = google_search_url(school_input, api_key)
        if target_url:
            status.write(f"ğŸŒ è‡ªåŠ¨æ‰¾åˆ°å…¥å£: {target_url}")
        else:
            status.update(label="âŒ æœç´¢å¤±è´¥", state="error")
            st.error("æœªæ‰¾åˆ°ç›¸å…³ç½‘é¡µï¼Œè¯·å°è¯•æ‰‹åŠ¨è¾“å…¥ URLã€‚")
            st.stop()
    else:
        st.warning("è¯·è¾“å…¥å­¦æ ¡åç§°æˆ–ç›®æ ‡ URL")
        st.stop()

    if target_url:
        status.write("ğŸ•µï¸ æ­£åœ¨å¯åŠ¨äº‘ç«¯æµè§ˆå™¨æŠ“å–...")
        raw_dbs = smart_crawl_and_extract(target_url)
        status.write(f"ğŸ“¦ åŸå§‹æå–æ¡ç›®æ•°: {len(raw_dbs)}")
        
        final_dbs = clean_data(raw_dbs)
        cn_dbs = sorted([d for d in final_dbs if is_chinese(d)])
        en_dbs = sorted([d for d in final_dbs if not is_chinese(d)])
        total = len(cn_dbs) + len(en_dbs)
        
        status.update(label="âœ… åˆ†æå®Œæˆï¼", state="complete", expanded=False)
        
        st.divider()
        st.markdown(f"### ğŸ“Š åˆ†ææŠ¥å‘Š: {school_input if school_input else 'è‡ªå®šä¹‰é“¾æ¥'}")
        st.caption(f"æ•°æ®æ¥æº: [{target_url}]({target_url})")
        
        if total == 0:
            st.error("âš ï¸ æœªæå–åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥ URL æˆ–ç½‘ç»œã€‚")
        else:
            c1, c2, c3 = st.columns(3)
            c1.metric("ğŸ“š æ€»è®¡èµ„æº", total)
            c2.metric("ğŸ‡¨ğŸ‡³ ä¸­æ–‡æ•°æ®åº“", len(cn_dbs))
            c3.metric("ğŸŒ å¤–æ–‡æ•°æ®åº“", len(en_dbs))
            
            st.divider()
            c_left, c_right = st.columns(2)
            with c_left:
                st.subheader("ä¸­æ–‡æ•°æ®åº“")
                if cn_dbs:
                    df_cn = pd.DataFrame(cn_dbs, columns=["åç§°"])
                    df_cn.index += 1
                    st.dataframe(df_cn, use_container_width=True)
            with c_right:
                st.subheader("å¤–æ–‡æ•°æ®åº“")
                if en_dbs:
                    df_en = pd.DataFrame(en_dbs, columns=["åç§°"])
                    df_en.index += 1
                    st.dataframe(df_en, use_container_width=True)