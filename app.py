import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import json

# --- æ ¸å¿ƒé€»è¾‘ ---

def google_search_url(school_name, api_key):
    """
    ä½¿ç”¨ Serper API (Google) æœç´¢ï¼Œå¼ºåˆ¶å¤„ç†ä¸­æ–‡ç¼–ç 
    """
    url = "https://google.serper.dev/search"
    
    # ç­–ç•¥ï¼šå°è¯•ä¸¤ä¸ªä¸åŒçš„æœç´¢è¯ï¼Œæé«˜å‘½ä¸­ç‡
    queries = [
        f"{school_name} å›¾ä¹¦é¦† æ•°æ®åº“ åˆ—è¡¨",  # ç²¾å‡†æœç´¢
        f"{school_name} å›¾ä¹¦é¦† è¯•ç”¨æ•°æ®åº“",    # å¤‡ç”¨æœç´¢
    ]
    
    headers = {
        'X-API-KEY': api_key,
        'Content-Type': 'application/json; charset=utf-8' # æ˜¾å¼å£°æ˜ UTF-8
    }

    for query in queries:
        try:
            print(f"æ­£åœ¨å°è¯•æœç´¢: {query}")
            # å…³é”®ä¿®å¤ï¼šä½¿ç”¨ json.dumps å¹¶ç¼–ç ä¸º utf-8 bytesï¼Œé˜²æ­¢ latin-1 æŠ¥é”™
            payload = json.dumps({
                "q": query,
                "gl": "cn",
                "hl": "zh-cn"
            }, ensure_ascii=False).encode('utf-8')

            response = requests.post(url, headers=headers, data=payload, timeout=10)
            
            if response.status_code == 200:
                results = response.json()
                # ä¼˜å…ˆæ‰¾ organic (è‡ªç„¶æœç´¢ç»“æœ)
                if 'organic' in results and len(results['organic']) > 0:
                    top_link = results['organic'][0]['link']
                    return top_link
            else:
                print(f"API çŠ¶æ€ç é”™è¯¯: {response.status_code}")
                
        except Exception as e:
            print(f"æœç´¢è¿‡ç¨‹æŠ¥é”™: {e}")
            continue # æ¢ä¸‹ä¸€ä¸ªè¯è¯•è¯•
            
    return None

def is_chinese(string):
    """åˆ¤æ–­æ˜¯å¦åŒ…å«ä¸­æ–‡"""
    for char in string:
        if '\u4e00' <= char <= '\u9fa5':
            return True
    return False

def analyze_page(url):
    """æŠ“å–å¹¶åˆ†æé¡µé¢"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, 'html.parser')

        # æ¸…ç†å¹²æ‰°é¡¹
        for tag in soup(['header', 'footer', 'nav', 'script', 'style', 'noscript']):
            tag.decompose()
        
        links = soup.find_all('a')
        db_list = []
        
        for link in links:
            text = link.get_text(strip=True)
            # æ™ºèƒ½è¿‡æ»¤ï¼šä¿ç•™é•¿åº¦é€‚ä¸­çš„é“¾æ¥æ–‡æœ¬
            if 3 < len(text) < 50: 
                db_list.append(text)
        
        # å»é‡
        db_list = list(set(db_list))
        
        cn_dbs = [db for db in db_list if is_chinese(db)]
        other_dbs = [db for db in db_list if not is_chinese(db)]
                
        return cn_dbs, other_dbs

    except Exception as e:
        st.error(f"æ— æ³•è¯»å–è¯¥å­¦æ ¡é¡µé¢ï¼ŒåŸå› : {e}")
        return None, None

# --- UI ç•Œé¢ ---

st.set_page_config(page_title="é«˜æ ¡æ•°æ®åº“æ™ºèƒ½ç»Ÿè®¡", page_icon="ğŸ•µï¸")

st.title("ğŸ•µï¸ é«˜æ ¡æ•°æ®åº“å…¨è‡ªåŠ¨ç»Ÿè®¡")
st.markdown("é›†æˆ **Google Search API**ï¼Œè‡ªåŠ¨å¯»æ‰¾æ•°æ®åº“åˆ—è¡¨ã€‚")

# ä¾§è¾¹æ 
with st.sidebar:
    st.header("âš™ï¸ è®¾ç½®")
    api_key = st.text_input("è¯·è¾“å…¥ Serper API Key", type="password")
    st.markdown("[ğŸ‘‰ ç‚¹å‡»è·å–å…è´¹ Key](https://serper.dev/)")
    st.divider()
    st.caption("å¦‚æœæ²¡æœ‰ Keyï¼Œæˆ–è€…è‡ªåŠ¨æœç´¢å¤±è´¥ï¼Œä½ ä¾ç„¶å¯ä»¥åœ¨å³ä¾§æ‰‹åŠ¨è¾“å…¥ç½‘å€ã€‚")

school_input = st.text_input("è¯·è¾“å…¥å­¦æ ¡å…¨ç§°ï¼ˆä¾‹å¦‚ï¼šé™•è¥¿å¸ˆèŒƒå¤§å­¦ï¼‰")

# åˆå§‹åŒ– session state ç”¨äºå­˜å‚¨æ‰¾åˆ°çš„ URL
if 'target_url' not in st.session_state:
    st.session_state.target_url = ""

# ä¸¤ä¸ªæŒ‰é’®é€»è¾‘
col_btn1, col_btn2 = st.columns([1, 2])
with col_btn1:
    auto_search = st.button("ğŸš€ å¼€å§‹å…¨è‡ªåŠ¨åˆ†æ", type="primary")

# --- ä¸»é€»è¾‘ ---

# 1. å¦‚æœç‚¹å‡»äº†è‡ªåŠ¨æœç´¢
if auto_search:
    if not api_key:
        st.error("è¯·å…ˆåœ¨å·¦ä¾§ä¾§è¾¹æ å¡«å…¥ API Keyï¼")
    elif not school_input:
        st.warning("è¯·å…ˆè¾“å…¥å­¦æ ¡åç§°")
    else:
        with st.status("ğŸ¤– æ­£åœ¨æŒ‡æŒ¥ Google æœç´¢...", expanded=True) as status:
            found_url = google_search_url(school_input, api_key)
            
            if found_url:
                status.update(label=f"âœ… æˆåŠŸæ‰¾åˆ°åœ°å€: {found_url}", state="complete", expanded=False)
                st.session_state.target_url = found_url # å­˜å…¥ç¼“å­˜
            else:
                status.update(label="âš ï¸ è‡ªåŠ¨æœç´¢æœªå‘½ä¸­", state="error", expanded=True)
                st.warning("Google æš‚æ—¶æ²¡æ‰¾åˆ°è¯¥å­¦æ ¡çš„æ•°æ®åº“åˆ—è¡¨é¡µï¼Œè¯·æ‰‹åŠ¨å°è¯•ã€‚")

# 2. å§‹ç»ˆæ˜¾ç¤ºçš„æ‰‹åŠ¨è¾“å…¥æ¡† (ä½œä¸ºå…œåº•)
st.divider()
st.markdown("##### ğŸ”— ç›®æ ‡ç½‘å€ç¡®è®¤")
user_url = st.text_input(
    "å¦‚æœä¸Šæ–¹è‡ªåŠ¨æœç´¢å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨ç²˜è´´è¯¥å­¦æ ¡ã€æ•°æ®åº“åˆ—è¡¨ã€‘ç½‘å€ï¼š", 
    value=st.session_state.target_url
)

# 3. å¦‚æœæœ‰ç½‘å€äº†ï¼Œå°±è¿›è¡Œåˆ†æ
if user_url:
    if st.button("å¼€å§‹æŠ“å–æ•°æ®"):
        with st.spinner(f"æ­£åœ¨è¯»å–ç½‘é¡µ: {user_url}"):
            cn_list, en_list = analyze_page(user_url)
            
            if cn_list is not None:
                st.success(f"ğŸ“Š åˆ†æå®Œæˆï¼å…±å‘ç° {len(cn_list) + len(en_list)} ä¸ªæ•°æ®åº“")
                
                col1, col2, col3 = st.columns(3)
                col1.metric("ä¸­æ–‡æ•°æ®åº“", f"{len(cn_list)}")
                col2.metric("å¤–æ–‡/å…¶ä»–", f"{len(en_list)}")
                col3.metric("æ€»è®¡", f"{len(cn_list) + len(en_list)}")
                
                tab1, tab2 = st.tabs(["ğŸ“ ä¸­æ–‡åº“æ¸…å•", "ğŸŒ å¤–æ–‡åº“æ¸…å•"])
                with tab1:
                    st.dataframe(pd.DataFrame(cn_list, columns=["æ•°æ®åº“åç§°"]), use_container_width=True)
                with tab2:
                    st.dataframe(pd.DataFrame(en_list, columns=["æ•°æ®åº“åç§°"]), use_container_width=True)