import streamlit as st
import subprocess
import os
import json
import requests
import time
from bs4 import BeautifulSoup
import pandas as pd

# --- 1. ç¯å¢ƒè‡ªæ£€ä¸åˆå§‹åŒ– (Playwright) ---
if "playwright_installed" not in st.session_state:
    # æ£€æŸ¥æ˜¯å¦å®‰è£…äº†æµè§ˆå™¨å†…æ ¸
    if not os.path.exists(os.path.expanduser("~/.cache/ms-playwright")):
        with st.spinner("æ­£åœ¨åˆå§‹åŒ–äº‘ç«¯æµè§ˆå™¨ç»„ä»¶... (é¦–æ¬¡è¿è¡Œçº¦éœ€1åˆ†é’Ÿ)"):
            subprocess.run(["playwright", "install", "chromium"])
    st.session_state.playwright_installed = True

# å°è¯•å¯¼å…¥ï¼Œå¦‚æœå¤±è´¥åˆ™è¯´æ˜requirements.txtæ²¡ç”Ÿæ•ˆï¼Œä½†é€šå¸¸è¿™æ—¶å·²ç»ç”Ÿæ•ˆäº†
try:
    from playwright.sync_api import sync_playwright
except ImportError:
    st.error("ä¸¥é‡é”™è¯¯ï¼šæœªæ‰¾åˆ° playwright åº“ã€‚è¯·æ£€æŸ¥ requirements.txt æ˜¯å¦åŒ…å« 'playwright'")
    st.stop()

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

def get_api_key():
    """ä»åå°è¯»å– API Key"""
    try:
        return st.secrets["SERPER_API_KEY"]
    except Exception:
        # å¦‚æœåå°æ²¡é…ï¼Œå…è®¸ä¸´æ—¶è¾“å…¥ï¼ˆæ–¹ä¾¿è°ƒè¯•ï¼‰
        return None

def google_search_url(school_name, api_key):
    """ã€å¤§è„‘ã€‘åˆ©ç”¨ Google æœç´¢æ‰¾åˆ°ç›®æ ‡ç½‘å€"""
    url = "https://google.serper.dev/search"
    queries = [
        f"{school_name} å›¾ä¹¦é¦† æ•°æ®åº“ åˆ—è¡¨",
        f"{school_name} å›¾ä¹¦é¦† è¯•ç”¨æ•°æ®åº“",
        f"{school_name} library database list"
    ]
    
    headers = {
        'X-API-KEY': api_key,
        'Content-Type': 'application/json; charset=utf-8'
    }

    for query in queries:
        try:
            payload = json.dumps({
                "q": query, 
                "gl": "cn", 
                "hl": "zh-cn"
            }, ensure_ascii=False).encode('utf-8')

            response = requests.post(url, headers=headers, data=payload, timeout=10)
            if response.status_code == 200:
                results = response.json()
                if 'organic' in results and len(results['organic']) > 0:
                    # è¿”å›æ‰¾åˆ°çš„ç¬¬ä¸€ä¸ªé“¾æ¥
                    found_link = results['organic'][0]['link']
                    return found_link
        except Exception as e:
            print(f"æœç´¢æŠ¥é”™: {e}")
            continue
    return None

def get_dynamic_page_content(url):
    """ã€å››è‚¢ã€‘åˆ©ç”¨ Playwright æ¸²æŸ“åŠ¨æ€ç½‘é¡µ"""
    with sync_playwright() as p:
        try:
            browser = p.chromium.launch(headless=True)
            # ä¼ªè£…æˆ Mac ç”µè„‘ä¸Šçš„ Chromeï¼Œé˜²æ­¢è¢«è¯†åˆ«ä¸ºçˆ¬è™«
            context = browser.new_context(
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )
            page = context.new_page()
            
            # è®¿é—®é¡µé¢
            page.goto(url, timeout=60000, wait_until="domcontentloaded")
            
            # æ™ºèƒ½ç­‰å¾…ï¼šå¼ºåˆ¶ç­‰å¾…å‡ ç§’è®© JS é£ä¸€ä¼šå„¿
            time.sleep(5)
            
            content = page.content()
            browser.close()
            return content
        except Exception as e:
            print(f"Playwright æŠ“å–å¤±è´¥: {e}")
            return None

def is_chinese(string):
    for char in string:
        if '\u4e00' <= char <= '\u9fa5':
            return True
    return False

def analyze_html(html_content):
    if not html_content:
        return [], []
    
    soup = BeautifulSoup(html_content, 'html.parser')
    for tag in soup(['header', 'footer', 'nav', 'script', 'style', 'noscript']):
        tag.decompose()
    
    links = soup.find_all('a')
    db_list = []
    for link in links:
        text = link.get_text(strip=True)
        if 2 < len(text) < 60: 
            db_list.append(text)
    
    db_list = list(set(db_list))
    cn_dbs = [db for db in db_list if is_chinese(db)]
    other_dbs = [db for db in db_list if not is_chinese(db)]
    return cn_dbs, other_dbs

# --- 3. UI ç•Œé¢ ---

st.set_page_config(page_title="é«˜æ ¡æ•°æ®åº“è‡ªåŠ¨ç»Ÿè®¡", page_icon="ğŸ«", layout="centered")

st.title("ğŸ« é«˜æ ¡æ•°æ®åº“å…¨è‡ªåŠ¨ç»Ÿè®¡")
st.caption("è¾“å…¥æ ¡å -> è‡ªåŠ¨æœç´¢ç½‘å€ -> è‡ªåŠ¨æ¸²æŸ“åŠ¨æ€é¡µé¢ -> è‡ªåŠ¨ç»Ÿè®¡")

# è·å– Key
api_key = get_api_key()

# è¾“å…¥åŒº
col1, col2 = st.columns([3, 1])
with col1:
    school_input = st.text_input("è¯·è¾“å…¥å­¦æ ¡å…¨ç§°", placeholder="ä¾‹å¦‚ï¼šé™•è¥¿å¸ˆèŒƒå¤§å­¦", label_visibility="collapsed")
with col2:
    start_btn = st.button("å¼€å§‹åˆ†æ", type="primary", use_container_width=True)

# çŠ¶æ€åŒº
status_box = st.status("ç­‰å¾…æŒ‡ä»¤...", expanded=False)

if start_btn:
    if not school_input:
        st.toast("è¯·è¾“å…¥æ ¡åï¼")
    elif not api_key:
        st.error("âŒ æœªæ£€æµ‹åˆ° API Keyã€‚è¯·åœ¨ Streamlit Cloud åå° Settings -> Secrets ä¸­é…ç½® SERPER_API_KEYã€‚")
    else:
        # ç¬¬ä¸€æ­¥ï¼šæœç´¢
        status_box.update(label=f"ğŸ” æ­£åœ¨å…¨ç½‘æœç´¢ã€{school_input}ã€‘çš„æ•°æ®åº“åˆ—è¡¨...", state="running", expanded=True)
        target_url = google_search_url(school_input, api_key)
        
        if target_url:
            status_box.write(f"âœ… æ‰¾åˆ°ç›®æ ‡åœ°å€: {target_url}")
            
            # ç¬¬äºŒæ­¥ï¼šæŠ“å–
            status_box.update(label="ğŸš€ æ­£åœ¨å¯åŠ¨äº‘ç«¯æµè§ˆå™¨åŠ è½½é¡µé¢ (å¯èƒ½éœ€è¦åå‡ ç§’)...", state="running")
            html_content = get_dynamic_page_content(target_url)
            
            if html_content:
                # ç¬¬ä¸‰æ­¥ï¼šåˆ†æ
                status_box.write("âœ… é¡µé¢åŠ è½½æˆåŠŸï¼Œæ­£åœ¨è§£æ...")
                cn_list, en_list = analyze_html(html_content)
                status_box.update(label="åˆ†æå®Œæˆï¼", state="complete", expanded=False)
                
                # ç¬¬å››æ­¥ï¼šå±•ç¤º
                total = len(cn_list) + len(en_list)
                st.divider()
                st.markdown(f"### ğŸ“Š {school_input}")
                st.caption(f"æ•°æ®æ¥æº: {target_url}")
                
                m1, m2, m3 = st.columns(3)
                m1.metric("æ€»è®¡", total)
                m2.metric("ä¸­æ–‡æ•°æ®åº“", len(cn_list))
                m3.metric("å¤–æ–‡æ•°æ®åº“", len(en_list))
                
                with st.expander("æŸ¥çœ‹è¯¦ç»†æ¸…å•"):
                    tab1, tab2 = st.tabs(["ä¸­æ–‡åº“", "å¤–æ–‡åº“"])
                    with tab1:
                        st.dataframe(pd.DataFrame(cn_list, columns=["åç§°"]), use_container_width=True, hide_index=True)
                    with tab2:
                        st.dataframe(pd.DataFrame(en_list, columns=["åç§°"]), use_container_width=True, hide_index=True)
            else:
                status_box.update(label="âŒ æµè§ˆå™¨åŠ è½½é¡µé¢å¤±è´¥", state="error")
                st.error("ç½‘é¡µåŠ è½½è¶…æ—¶æˆ–è¢«åçˆ¬è™«æ‹¦æˆªï¼Œæ— æ³•è·å–å†…å®¹ã€‚")
        else:
            status_box.update(label="âŒ è‡ªåŠ¨æœç´¢å¤±è´¥", state="error")
            st.warning("Google æœªèƒ½æ‰¾åˆ°è¯¥å­¦æ ¡æ˜ç¡®çš„æ•°æ®åº“åˆ—è¡¨é¡µé¢ã€‚")
            manual_url = st.text_input("è¯·æ‰‹åŠ¨ç²˜è´´ç½‘å€å°è¯•ï¼š")