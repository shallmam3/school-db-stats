import streamlit as st
import subprocess
import os

# --- å…³é”®ä¿®å¤ï¼šäº‘ç«¯è‡ªåŠ¨å®‰è£… Playwright æµè§ˆå™¨ ---
# è¿™æ®µä»£ç ä¼šæ£€æŸ¥æ˜¯å¦åœ¨äº‘ç«¯ï¼Œå¦‚æœæ˜¯ï¼Œå°±è‡ªåŠ¨ä¸‹è½½æµè§ˆå™¨
try:
    from playwright.sync_api import sync_playwright
except ImportError:
    # å¦‚æœåº“éƒ½æ²¡è£…ä¸Šï¼Œå°è¯•å¼ºåˆ¶å®‰è£…ï¼ˆé€šå¸¸ requirements.txt ä¼šæå®šï¼Œè¿™é‡Œæ˜¯å…œåº•ï¼‰
    subprocess.check_call([os.sys.executable, "-m", "pip", "install", "playwright"])
    from playwright.sync_api import sync_playwright

# æ¯æ¬¡å¯åŠ¨æ—¶ç¡®ä¿æµè§ˆå™¨å·²å®‰è£…
# æ³¨æ„ï¼šè¿™ä¼šå¢åŠ ä¸€ç‚¹å¯åŠ¨æ—¶é—´ï¼Œä½†åœ¨äº‘ç«¯æ˜¯å¿…é¡»çš„
subprocess.run(["playwright", "install", "chromium"])

# ------------------------------------------------
# ä¸‹é¢æ˜¯ä½ åŸæœ¬çš„ä»£ç ...
from bs4 import BeautifulSoup
import pandas as pd
import time

def get_dynamic_page_content(url):
    """
    ä½¿ç”¨ Playwright åŠ è½½åŠ¨æ€ç½‘é¡µ
    """
    with sync_playwright() as p:
        # ä½¿ç”¨ chromium æµè§ˆå™¨
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        
        try:
            status_text = st.empty()
            status_text.text(f"æ­£åœ¨æ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®: {url} ...")
            
            page.goto(url, timeout=60000) # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°60ç§’
            
            # ç­‰å¾…ç½‘ç»œç©ºé—²ï¼Œè¡¨ç¤ºåŠ è½½å®Œæˆ
            page.wait_for_load_state("networkidle") 
            
            # é¢å¤–ç­‰å¾…ç¡®ä¿ JS æ‰§è¡Œ
            time.sleep(3)
            
            content = page.content()
            status_text.empty()
            return content
            
        except Exception as e:
            st.error(f"åŠ è½½å¤±è´¥: {e}")
            return None
        finally:
            browser.close()

# ... (åé¢ is_chinese, analyze_html å’Œ UI éƒ¨åˆ†ä¿æŒä¸å˜ï¼Œç›´æ¥å¤åˆ¶ä½ ä¹‹å‰çš„å³å¯)
# ä¸ºäº†æ–¹ä¾¿ï¼Œæˆ‘æŠŠåé¢çš„ UI éƒ¨åˆ†ä¹Ÿç®€ç•¥å†™åœ¨è¿™é‡Œï¼Œä½ å¯ä»¥ç›´æ¥ä¿ç•™ä½ ä¹‹å‰çš„
def is_chinese(string):
    for char in string:
        if '\u4e00' <= char <= '\u9fa5':
            return True
    return False

def analyze_html(html_content):
    if not html_content:
        return [], []
    soup = BeautifulSoup(html_content, 'html.parser')
    for tag in soup(['header', 'footer', 'nav', 'script', 'style', 'noscript']):
        tag.decompose()
    links = soup.find_all('a')
    db_list = []
    for link in links:
        text = link.get_text(strip=True)
        if 2 < len(text) < 50: 
            db_list.append(text)
    db_list = list(set(db_list))
    cn_dbs = [db for db in db_list if is_chinese(db)]
    other_dbs = [db for db in db_list if not is_chinese(db)]
    return cn_dbs, other_dbs

# --- UI ---
st.set_page_config(page_title="åŠ¨æ€ç½‘é¡µæ•°æ®åº“æŠ“å–", page_icon="ğŸ•µï¸")
st.title("ğŸ•µï¸ è¶…æ˜Ÿ/åŠ¨æ€ç½‘é¡µæŠ“å–åŠ©æ‰‹")

target_url = st.text_input("è¯·è¾“å…¥ç½‘å€ï¼š", value="http://wisdom.chaoxing.com/newwisdom/doordatabase/database.html?pageId=48038&wfwfid=1803&sw=")

if st.button("å¼€å§‹å¼ºåŠ›æŠ“å–"):
    if not target_url:
        st.warning("è¯·å…ˆè¾“å…¥ç½‘å€")
    else:
        with st.status("ğŸš€ æ­£åœ¨å¯åŠ¨ä»¿çœŸæµè§ˆå™¨...", expanded=True) as status:
            html_content = get_dynamic_page_content(target_url)
            if html_content:
                status.write("âœ… é¡µé¢åŠ è½½æˆåŠŸï¼æ­£åœ¨è§£æ...")
                cn_list, en_list = analyze_html(html_content)
                status.update(label="åˆ†æå®Œæˆï¼", state="complete", expanded=False)
                
                col1, col2, col3 = st.columns(3)
                col1.metric("ä¸­æ–‡æ•°æ®åº“", f"{len(cn_list)}")
                col2.metric("å¤–æ–‡/å…¶ä»–", f"{len(en_list)}")
                col3.metric("æ€»è®¡", f"{len(cn_list) + len(en_list)}")
                
                tab1, tab2 = st.tabs(["ğŸ“ ä¸­æ–‡åº“æ¸…å•", "ğŸŒ å¤–æ–‡åº“æ¸…å•"])
                with tab1:
                    st.dataframe(pd.DataFrame(cn_list, columns=["åç§°"]), use_container_width=True)
                with tab2:
                    st.dataframe(pd.DataFrame(en_list, columns=["åç§°"]), use_container_width=True)