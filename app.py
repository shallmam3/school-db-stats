import streamlit as st
import subprocess
import os
import json
import requests
import re
from bs4 import BeautifulSoup
import pandas as pd

# --- 1. ç¯å¢ƒä¸ä¾èµ– ---
# æ—¢ç„¶æ˜¯è¯»æ–‡ç« ï¼Œä¸éœ€è¦ç¬¨é‡çš„ Playwright äº†ï¼Œæ™®é€šçš„ requests å°±å¤Ÿäº†ï¼Œé€Ÿåº¦æ›´å¿«
if "playwright_installed" not in st.session_state:
    st.session_state.playwright_installed = True

# --- 2. æ ¸å¿ƒï¼šä¸»æµæ•°æ®åº“è¯å…¸ (ä½ å¯ä»¥éšæ—¶è¡¥å……) ---
# è¿™æ˜¯æˆ‘ä»¬åœ¨æ–‡ç« ä¸­â€œå¯»æ‰¾â€çš„ç›®æ ‡
COMMON_DBS = {
    "CN": [
        "ä¸­å›½çŸ¥ç½‘", "CNKI", "ä¸‡æ–¹", "ç»´æ™®", "è¶…æ˜Ÿ", "è¯»ç§€", "é¾™æº", 
        "äººå¤§å¤å°", "CSCD", "CSSCI", "ä¸­ååŒ»å­¦", "å›½ç ”ç½‘", "EPSæ•°æ®", 
        "æ–°ä¸œæ–¹", "é“¶ç¬¦", "èµ·ç‚¹è€ƒè¯•", "ä¸­ç§‘", "ä¼˜é˜…", "ä¹¦ç”Ÿä¹‹å®¶"
    ],
    "EN": [
        "Web of Science", "WOS", "SCI", "SSCI", "EI", "Engineering Village", 
        "ScienceDirect", "Elsevier", "Springer", "Wiley", "IEEE", "IEL", 
        "Nature", "Science", "ACS", "RSC", "ProQuest", "EBSCO", "JSTOR", 
        "PubMed", "Embase", "Scopus", "Taylor", "Francis", "SAGE", 
        "Emerald", "ACM", "ASCE", "ASME", "LexisNexis", "Westlaw"
    ]
}

def get_api_key():
    try:
        return st.secrets["SERPER_API_KEY"]
    except:
        return None

def google_search_articles(school_name, api_key):
    """
    æœç´¢ç­–ç•¥è½¬å˜ï¼šæ‰¾æ–‡ç« ã€æ‰¾æŒ‡å—ã€æ‰¾æ¦‚è§ˆ
    """
    url = "https://google.serper.dev/search"
    queries = [
        f"{school_name} å›¾ä¹¦é¦† \"æ•°å­—èµ„æº\" å¯¼è§ˆ",
        f"{school_name} å›¾ä¹¦é¦† \"å·²è´­æ•°æ®åº“\" ä¸€è§ˆ",
        f"{school_name} å›¾ä¹¦é¦† æ–°ç”Ÿå…¥é¦†æŒ‡å— èµ„æºä»‹ç»",
        f"site:mp.weixin.qq.com {school_name} å›¾ä¹¦é¦† æ•°æ®åº“" # ä¸“é—¨æœå¾®ä¿¡æ¨æ–‡
    ]
    
    headers = {'X-API-KEY': api_key, 'Content-Type': 'application/json; charset=utf-8'}

    links = []
    for query in queries:
        try:
            payload = json.dumps({"q": query, "gl": "cn", "hl": "zh-cn"}, ensure_ascii=False).encode('utf-8')
            response = requests.post(url, headers=headers, data=payload, timeout=5)
            if response.status_code == 200:
                results = response.json()
                if 'organic' in results:
                    # å–å‰3ä¸ªç»“æœï¼Œå¢åŠ å‘½ä¸­ç‡
                    for item in results['organic'][:3]:
                        links.append({
                            "title": item.get('title'),
                            "link": item.get('link'),
                            "snippet": item.get('snippet')
                        })
        except:
            continue
    
    # å»é‡
    seen = set()
    unique_links = []
    for l in links:
        if l['link'] not in seen:
            unique_links.append(l)
            seen.add(l['link'])
    return unique_links[:5] # æœ€å¤šåˆ†æ5ç¯‡

def analyze_page_content(url):
    """
    æŠ“å–æ–‡ç« å†…å®¹å¹¶è¿›è¡Œâ€œè¯å…¸åŒ¹é…â€
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        resp.encoding = resp.apparent_encoding # è‡ªåŠ¨çº æ­£ç¼–ç 
        text = BeautifulSoup(resp.text, 'html.parser').get_text()
        
        found_cn = set()
        found_en = set()
        
        # 1. æ‰«æä¸­æ–‡åº“
        for db in COMMON_DBS["CN"]:
            # ç®€å•çš„ä¸åŒºåˆ†å¤§å°å†™åŒ¹é…
            if db.lower() in text.lower():
                found_cn.add(db)
                
        # 2. æ‰«æå¤–æ–‡åº“
        for db in COMMON_DBS["EN"]:
            # å•è¯è¾¹ç•ŒåŒ¹é…é˜²æ­¢è¯¯åˆ¤ (ä¾‹å¦‚æœ EI ä¸åŒ¹é… height)
            if re.search(r'\b' + re.escape(db) + r'\b', text, re.IGNORECASE) or db in text:
                found_en.add(db)
                
        return list(found_cn), list(found_en)
    except Exception as e:
        return [], []

# --- 3. UI ç•Œé¢ ---
st.set_page_config(page_title="é«˜æ ¡èµ„æºæƒ…æŠ¥åˆ†æ", page_icon="ğŸ•µï¸", layout="wide")

st.title("ğŸ•µï¸ é«˜æ ¡æ•°æ®åº“èµ„æºæƒ…æŠ¥åˆ†æ")
st.caption("æ€è·¯ï¼šé€šè¿‡æœç´¢å…¬å¼€çš„â€œå…¥é¦†æŒ‡å—â€ã€â€œèµ„æºå¯¼è§ˆâ€æˆ–â€œæ–°é—»é€šå‘Šâ€ï¼ŒåŒ¹é…ä¸»æµæ•°æ®åº“åå•ã€‚")

with st.sidebar:
    st.header("âš™ï¸ è®¾ç½®")
    api_key = st.text_input("SERPER_API_KEY", value=get_api_key() or "", type="password")
    
school_input = st.text_input("è¯·è¾“å…¥å­¦æ ¡å…¨ç§°", placeholder="ä¾‹å¦‚ï¼šè¥¿å®‰ç§‘æŠ€å¤§å­¦")
run_btn = st.button("å¼€å§‹ä¾¦å¯Ÿ", type="primary")

if run_btn:
    if not api_key:
        st.error("è¯·é…ç½® SERPER_API_KEY")
        st.stop()
    if not school_input:
        st.warning("è¯·è¾“å…¥æ ¡å")
        st.stop()

    status = st.status("ğŸ” æ­£åœ¨å…¨ç½‘æœç´¢ç›¸å…³æƒ…æŠ¥...", expanded=True)
    
    # 1. æœç´¢æ–‡ç« 
    articles = google_search_articles(school_input, api_key)
    
    if not articles:
        status.update(label="âŒ æœªæ‰¾åˆ°å…¬å¼€æƒ…æŠ¥", state="error")
        st.error("æœªæœç´¢åˆ°ç›¸å…³æ–‡ç« ï¼Œè¯¥å­¦æ ¡å¯èƒ½è¾ƒå°‘å…¬å¼€è¯¦ç»†èµ„æºåˆ—è¡¨ã€‚")
    else:
        status.write(f"ğŸ“„ æ‰¾åˆ° {len(articles)} ç¯‡ç›¸å…³å…¬å¼€æ–‡æ¡£/æ–‡ç« ï¼Œå¼€å§‹åˆ†æå†…å®¹...")
        
        all_cn = set()
        all_en = set()
        valid_sources = []
        
        # 2. é€ç¯‡åˆ†æ
        progress_bar = status.progress(0)
        for i, article in enumerate(articles):
            status.write(f"æ­£åœ¨é˜…è¯»: [{article['title']}]...")
            cn, en = analyze_page_content(article['link'])
            
            if cn or en:
                all_cn.update(cn)
                all_en.update(en)
                valid_sources.append(article)
            
            progress_bar.progress((i + 1) / len(articles))
            
        status.update(label="âœ… åˆ†æå®Œæˆï¼", state="complete", expanded=False)
        
        # --- ç»“æœå±•ç¤º ---
        st.divider()
        total = len(all_cn) + len(all_en)
        
        # é¡¶éƒ¨ KPI
        c1, c2, c3 = st.columns(3)
        c1.metric("ç–‘ä¼¼å·²è´­èµ„æº", total, help="é€šè¿‡å…³é”®è¯åŒ¹é…åˆ°çš„ä¸»æµæ•°æ®åº“æ•°é‡")
        c2.metric("ä¸­æ–‡æ ¸å¿ƒ", len(all_cn))
        c3.metric("å¤–æ–‡æ ¸å¿ƒ", len(all_en))
        
        st.info(f"ğŸ’¡ åˆ†æç»“è®ºï¼šæ ¹æ®å…¬å¼€ä¿¡æ¯ï¼Œè¯¥æ ¡æå¤§æ¦‚ç‡æ‹¥æœ‰ä»¥ä¸‹èµ„æºã€‚æ•°æ®æ¥æºäºå¯¹ {len(valid_sources)} ç¯‡å…¬å¼€æ–‡ç« çš„æ–‡æœ¬åˆ†æã€‚")

        # è¯¦ç»†åˆ—è¡¨
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("ğŸ‡¨ğŸ‡³ ä¸­æ–‡èµ„æº (åŒ¹é…å‘½ä¸­)")
            if all_cn:
                # è½¬æ¢æˆ DataFrame æ˜¾ç¤ºæ›´å¥½çœ‹
                df_cn = pd.DataFrame(sorted(list(all_cn)), columns=["æ•°æ®åº“åç§°"])
                st.dataframe(df_cn, use_container_width=True, hide_index=True)
            else:
                st.text("æœªæ£€æµ‹åˆ°å¸¸è§ä¸­æ–‡åº“")
                
        with col2:
            st.subheader("ğŸŒ å¤–æ–‡èµ„æº (åŒ¹é…å‘½ä¸­)")
            if all_en:
                df_en = pd.DataFrame(sorted(list(all_en)), columns=["æ•°æ®åº“åç§°"])
                st.dataframe(df_en, use_container_width=True, hide_index=True)
            else:
                st.text("æœªæ£€æµ‹åˆ°å¸¸è§å¤–æ–‡åº“")

        st.divider()
        st.markdown("#### ğŸ”— è¯æ®æ¥æº (ç‚¹å‡»æŸ¥çœ‹åŸæ–‡)")
        for src in valid_sources:
            st.markdown(f"- [{src['title']}]({src['link']})")
            st.caption(f"æ‘˜è¦: {src['snippet']}")