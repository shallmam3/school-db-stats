import streamlit as st
import subprocess
import os
import json
import requests
import time
import re
from bs4 import BeautifulSoup
import pandas as pd
from collections import Counter

# --- 1. ç¯å¢ƒåˆå§‹åŒ– ---
if "playwright_installed" not in st.session_state:
    if not os.path.exists(os.path.expanduser("~/.cache/ms-playwright")):
        with st.spinner("æ­£åœ¨åˆå§‹åŒ–äº‘ç«¯æµè§ˆå™¨..."):
            subprocess.run(["playwright", "install", "chromium"])
    st.session_state.playwright_installed = True

try:
    from playwright.sync_api import sync_playwright
except ImportError:
    st.error("è¯·æ£€æŸ¥ requirements.txt æ˜¯å¦åŒ…å« playwright")
    st.stop()

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

def get_api_key():
    try:
        return st.secrets["SERPER_API_KEY"]
    except:
        return None

def google_search_url(school_name, api_key):
    """æœç´¢å…¥å£ URL"""
    url = "https://google.serper.dev/search"
    # ä¼˜åŒ–æœç´¢è¯ï¼Œç›´æ¥æ‰¾â€œå·²è´­èµ„æºâ€
    queries = [
        f"{school_name} å›¾ä¹¦é¦† å·²è´­èµ„æº åˆ—è¡¨",
        f"{school_name} å›¾ä¹¦é¦† æ•°æ®åº“å¯¼èˆª",
        f"{school_name} å›¾ä¹¦é¦† ç”µå­èµ„æº"
    ]
    
    headers = {'X-API-KEY': api_key, 'Content-Type': 'application/json; charset=utf-8'}

    for query in queries:
        try:
            payload = json.dumps({"q": query, "gl": "cn", "hl": "zh-cn"}, ensure_ascii=False).encode('utf-8')
            response = requests.post(url, headers=headers, data=payload, timeout=10)
            if response.status_code == 200:
                results = response.json()
                if 'organic' in results and len(results['organic']) > 0:
                    return results['organic'][0]['link']
        except:
            continue
    return None

def extract_from_table(soup):
    """
    ã€æ ¸å¿ƒå‡çº§ã€‘ä¸“é—¨è¯†åˆ«â€œè¡¨æ ¼â€ç»“æ„ (Target: çº¢æ¡†å†…çš„å†…å®¹)
    åªæœ‰åœ¨è¡¨æ ¼é‡Œçš„å†…å®¹æ‰ä¼šè¢«æå–ï¼Œå½»åº•å±è”½ä¾§è¾¹æ å¹²æ‰°ã€‚
    """
    db_list = []
    
    # æ‰¾åˆ°æ‰€æœ‰çš„è¡¨æ ¼
    tables = soup.find_all('table')
    
    for table in tables:
        # æ£€æŸ¥è¡¨å¤´ï¼Œç¡®è®¤æ˜¯ä¸æ˜¯æ•°æ®åº“åˆ—è¡¨
        # åªè¦è¡¨æ ¼æ–‡å­—é‡ŒåŒ…å«è¿™äº›å…³é”®è¯ï¼Œå°±è®¤ä¸ºæ˜¯ç›®æ ‡è¡¨æ ¼
        text_content = table.get_text()
        keywords = ["æ•°æ®åº“", "èµ„æºåç§°", "é¢˜å", "å·²è´­", "è®¢è´­", "ä¸­æ–‡", "å¤–æ–‡"]
        
        # è®¡ç®—åŒ¹é…åˆ°çš„å…³é”®è¯æ•°é‡
        match_count = sum(1 for k in keywords if k in text_content)
        
        # å¦‚æœå…³é”®è¯å¤ªå°‘ï¼Œè¯´æ˜è¿™å¯èƒ½åªæ˜¯ä¸ªæ’ç‰ˆè¡¨æ ¼ï¼Œè·³è¿‡
        if match_count < 2:
            continue
            
        # --- æå–è¡¨æ ¼å†…å®¹ ---
        # éå†æ‰€æœ‰è¡Œ
        rows = table.find_all('tr')
        for row in rows:
            # éå†æ‰€æœ‰å•å…ƒæ ¼
            cells = row.find_all(['td', 'th'])
            for cell in cells:
                # æå–é“¾æ¥æ–‡æœ¬
                links = cell.find_all('a')
                for link in links:
                    text = link.get_text(strip=True)
                    if 2 < len(text) < 60:
                        db_list.append(text)
                
                # å¦‚æœæ²¡æœ‰é“¾æ¥ï¼Œæœ‰æ—¶å€™æ˜¯çº¯æ–‡æœ¬(ä½†è¾ƒå°‘è§ï¼Œé€šå¸¸æ•°æ®åº“éƒ½æ˜¯é“¾æ¥)
                if not links:
                     text = cell.get_text(strip=True)
                     if 2 < len(text) < 60 and not text.isdigit():
                         db_list.append(text)

    return db_list

def smart_crawl_and_extract(url):
    """
    ã€æ™ºèƒ½æ¢è·¯è€…ã€‘
    1. åŠ è½½é¡µé¢
    2. å¦‚æœå½“å‰é¡µé¢ä¸åƒåˆ—è¡¨ï¼Œå°è¯•ç‚¹å‡»â€œå·²è´­èµ„æºâ€ç­‰æŒ‰é’®è·³è½¬
    3. æ¸²æŸ“æœ€ç»ˆé¡µé¢å¹¶æå–
    """
    with sync_playwright() as p:
        try:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Chrome/120.0.0.0 Safari/537.36')
            page = context.new_page()
            
            # 1. è®¿é—®åˆå§‹é¡µé¢
            page.goto(url, timeout=60000, wait_until="domcontentloaded")
            time.sleep(3) # ç­‰å¾…é¦–å±åŠ è½½
            
            # --- æ™ºèƒ½è·³è½¬é€»è¾‘ (Deep Navigation) ---
            # æ£€æŸ¥å½“å‰é¡µé¢æ˜¯å¦å·²ç»æ˜¯åˆ—è¡¨é¡µï¼ˆæœ‰æ²¡æœ‰â€œä¸­æ–‡æ•°æ®åº“â€ã€â€œå·²è´­â€ç­‰å­—æ ·ï¼‰
            content = page.content()
            if "ä¸­æ–‡æ•°æ®åº“" not in content and "å·²è´­" not in content:
                # å¦‚æœå½“å‰é¡µä¸åƒåˆ—è¡¨é¡µï¼Œå°è¯•å¯»æ‰¾â€œå…¥å£â€æŒ‰é’®å¹¶ç‚¹å‡»
                # æ¨¡ç³ŠåŒ¹é…é“¾æ¥æ–‡å­—
                potential_links = page.get_by_role("link").all()
                target_keywords = ["å·²è´­èµ„æº", "æ•°æ®åº“å¯¼èˆª", "ä¸­æ–‡æ•°æ®åº“", "æ‰€æœ‰æ•°æ®åº“", "è®¢è´­èµ„æº"]
                
                for link in potential_links:
                    try:
                        text = link.text_content()
                        if any(kw in text for kw in target_keywords):
                            print(f"ğŸ•µï¸ å‘ç°æ½œåœ¨å…¥å£: {text}ï¼Œæ­£åœ¨è·³è½¬...")
                            # æ‰¾åˆ°å…¥å£ï¼Œç‚¹å‡»å¹¶ç­‰å¾…åŠ è½½
                            with page.expect_navigation(timeout=15000):
                                link.click()
                            time.sleep(5) # ç­‰å¾…æ–°é¡µé¢åŠ è½½
                            break # åªè·³ä¸€æ¬¡
                    except:
                        pass

            # 2. è·å–æœ€ç»ˆé¡µé¢å†…å®¹
            final_content = page.content()
            browser.close()
            
            # --- è§£æé˜¶æ®µ ---
            soup = BeautifulSoup(final_content, 'html.parser')
            
            # ç­–ç•¥ A: ä¼˜å…ˆå°è¯•ä»è¡¨æ ¼(Table)æå– (æœ€ç²¾å‡†ï¼Œå¯¹åº”ä½ çš„æˆªå›¾)
            db_list = extract_from_table(soup)
            
            # ç­–ç•¥ B: å¦‚æœæ²¡æ‰¾åˆ°è¡¨æ ¼ï¼Œå›é€€åˆ°ä¹‹å‰çš„æ™ºèƒ½åŒºåŸŸæ³• (å…œåº•)
            if len(db_list) < 5:
                # (è¿™é‡Œå¤ç”¨ä¹‹å‰çš„é€»è¾‘ï¼Œä½œä¸ºå¤‡ç”¨)
                # æ¸…ç†å¹²æ‰°
                for tag in soup(['header', 'footer', 'nav', 'script', 'style', 'iframe', 'form']):
                    tag.decompose()
                
                # å¯»æ‰¾æœ€å¯†é›†çš„åŒºåŸŸ
                all_links = soup.find_all('a')
                parents = []
                for link in all_links:
                    if 2 < len(link.get_text(strip=True)) < 60:
                        parent = link.find_parent(['ul', 'div', 'tbody', 'section'])
                        if parent: parents.append(parent)
                
                if parents:
                    top_parent, count = Counter(parents).most_common(1)[0]
                    if count > 5:
                        for link in top_parent.find_all('a'):
                            db_list.append(link.get_text(strip=True))

            return db_list
            
        except Exception as e:
            print(f"Error: {e}")
            return []

def is_chinese(string):
    for char in string:
        if '\u4e00' <= char <= '\u9fa5': return True
    return False

def clean_data(raw_list):
    """æœ€åä¸€é“æ¸…æ´—å·¥åº"""
    blacklist = [
        "é¦–é¡µ", "ç™»å½•", "æ³¨å†Œ", "æ›´å¤š", "æŸ¥çœ‹", "è®¢è´­", "è¯•ç”¨", "ç®€ä»‹", "æŒ‡å—", 
        "è¯¦ç»†", "è®¿é—®", "æ ¡å¤–", "å’¨è¯¢", "åé¦ˆ", "ç‚¹å‡»", "ä¸‹è½½", "English",
        "åºå·", "çŠ¶æ€", "ç±»å‹", "åç§°", "æ•°æ®åº“åç§°", "æ“ä½œ", "æ¥æº" # è¡¨å¤´è¯ä¹Ÿè¦è¿‡æ»¤
    ]
    clean_list = []
    for item in raw_list:
        text = item.strip()
        if 2 < len(text) < 60 and not text.isdigit():
            if not any(junk in text for junk in blacklist):
                clean_list.append(text)
    return list(set(clean_list))

# --- 3. UI ç•Œé¢ ---
st.set_page_config(page_title="é«˜æ ¡æ•°æ®åº“ç»Ÿè®¡Pro", page_icon="ğŸ«", layout="centered")
st.title("ğŸ« é«˜æ ¡æ•°æ®åº“å…¨è‡ªåŠ¨ç»Ÿè®¡ (Proç‰ˆ)")
st.caption("æ™ºèƒ½è¯†åˆ«è¡¨æ ¼ç»“æ„ | è‡ªåŠ¨è·³è½¬äºŒçº§é¡µé¢")

api_key = get_api_key()
school_input = st.text_input("è¯·è¾“å…¥å­¦æ ¡å…¨ç§°", placeholder="ä¾‹å¦‚ï¼šè¥¿å®‰ç§‘æŠ€å¤§å­¦")
start_btn = st.button("å¼€å§‹æ·±åº¦åˆ†æ", type="primary")

status = st.status("å‡†å¤‡å°±ç»ª", expanded=False)

if start_btn:
    if not api_key:
        st.error("è¯·é…ç½® SERPER_API_KEY")
    elif not school_input:
        st.warning("è¯·è¾“å…¥æ ¡å")
    else:
        status.update(label="ğŸ” æ­£åœ¨å¯»æ‰¾æ•°æ®åº“å…¥å£...", state="running", expanded=True)
        url = google_search_url(school_input, api_key)
        
        if url:
            status.write(f"ğŸŒ åˆå§‹å…¥å£: {url}")
            status.write("ğŸ•µï¸ æ­£åœ¨å¯åŠ¨æµè§ˆå™¨ï¼Œå°è¯•å¯»æ‰¾è¡¨æ ¼æ•°æ® (åŒ…å«è‡ªåŠ¨è·³è½¬)...")
            
            # æ‰§è¡Œæ™ºèƒ½æŠ“å–
            raw_dbs = smart_crawl_and_extract(url)
            
            # æ¸…æ´—
            final_dbs = clean_data(raw_dbs)
            
            cn_dbs = [d for d in final_dbs if is_chinese(d)]
            en_dbs = [d for d in final_dbs if not is_chinese(d)]
            total = len(cn_dbs) + len(en_dbs)
            
            status.update(label="âœ… åˆ†æå®Œæˆï¼", state="complete", expanded=False)
            
            st.divider()
            st.markdown(f"### ğŸ“Š {school_input} åˆ†ææŠ¥å‘Š")
            st.caption(f"æ•°æ®æ¥æº: {url}")
            
            if total == 0:
                st.error("æœªæå–åˆ°æœ‰æ•ˆæ•°æ®ã€‚å¯èƒ½åŸå› ï¼šé¡µé¢éœ€è¦æ ¡å†…ç½‘(VPN)æ‰èƒ½çœ‹åˆ°è¡¨æ ¼ï¼Œæˆ–è€…åçˆ¬è™«éå¸¸ä¸¥æ ¼ã€‚")
            else:
                m1, m2, m3 = st.columns(3)
                m1.metric("æ€»è®¡", total)
                m2.metric("ä¸­æ–‡åº“", len(cn_dbs))
                m3.metric("å¤–æ–‡åº“", len(en_dbs))
                
                with st.expander("ğŸ“„ æŸ¥çœ‹è¯¦ç»†æ¸…å• (å·²å‰”é™¤ä¾§è¾¹æ å¹²æ‰°)", expanded=True):
                    c1, c2 = st.columns(2)
                    with c1:
                        st.dataframe(pd.DataFrame(cn_dbs, columns=["ä¸­æ–‡æ•°æ®åº“"]), use_container_width=True)
                    with c2:
                        st.dataframe(pd.DataFrame(en_dbs, columns=["å¤–æ–‡æ•°æ®åº“"]), use_container_width=True)
        else:
            status.update(label="âŒ æœç´¢å¤±è´¥", state="error")
            st.error("æœªæ‰¾åˆ°ç›¸å…³ç½‘é¡µã€‚")