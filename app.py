import streamlit as st
import requests
from bs4 import BeautifulSoup
from duckduckgo_search import DDGS
import pandas as pd
import time
import re
from urllib.parse import urlparse

# --- æ ¸å¿ƒé€»è¾‘å‡½æ•° ---

def search_library_url(school_name):
    """
    åˆ©ç”¨ DuckDuckGo æœç´¢å­¦æ ¡å›¾ä¹¦é¦†æ•°æ®åº“åˆ—è¡¨çš„ URL
    """
    query = f"{school_name} å›¾ä¹¦é¦† æ•°æ®åº“ åˆ—è¡¨"
    print(f"æ­£åœ¨æœç´¢: {query}")
    try:
        results = DDGS().text(query, max_results=3)
        if results:
            # è¿”å›ç¬¬ä¸€ä¸ªçœ‹èµ·æ¥åƒé“¾æ¥çš„ç»“æœ
            return results[0]['href']
    except Exception as e:
        st.error(f"æœç´¢å‡ºé”™: {e}")
    return None

def is_chinese(string):
    """åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦å«æœ‰ä¸­æ–‡"""
    for char in string:
        if '\u4e00' <= char <= '\u9fa5':
            return True
    return False

def analyze_page(url):
    """
    æŠ“å–å¹¶æ™ºèƒ½åˆ†æé¡µé¢å†…å®¹
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = response.apparent_encoding # è‡ªåŠ¨ä¿®å¤ä¹±ç 
        soup = BeautifulSoup(response.text, 'lxml')
        
        # --- æ™ºèƒ½è§£æç­–ç•¥ ---
        # éš¾ç‚¹ï¼šå¦‚ä½•ä»æ‚ä¹±çš„ç½‘é¡µä¸­åªæå–æ•°æ®åº“åå­—ï¼Ÿ
        # ç­–ç•¥ï¼šé€šå¸¸æ•°æ®åº“åˆ—è¡¨åœ¨ <li> æˆ– <tr> æ ‡ç­¾ä¸‹çš„ <a> æ ‡ç­¾ä¸­
        # æˆ‘ä»¬æå–é¡µé¢ä¸»è¦å†…å®¹åŒºåŸŸçš„é“¾æ¥
        
        # 1. å°è¯•ç§»é™¤å¯¼èˆªæ å’Œé¡µè„šï¼ˆç®€å•çš„æ¸…ç†ï¼‰
        for tag in soup(['header', 'footer', 'nav', 'script', 'style']):
            tag.decompose()
            
        # 2. æå–æ‰€æœ‰é“¾æ¥æ–‡æœ¬
        links = soup.find_all('a')
        
        db_list = []
        for link in links:
            text = link.get_text(strip=True)
            # è¿‡æ»¤æ‰æ— æ•ˆé“¾æ¥ï¼ˆå¦‚â€œé¦–é¡µâ€ã€â€œè”ç³»æˆ‘ä»¬è¦â€ã€â€œEnglishâ€ç­‰çŸ­è¯ï¼‰
            if len(text) > 3 and len(text) < 50: 
                # è¿™é‡Œå¯ä»¥åŠ æ›´å¤æ‚çš„å…³é”®è¯è¿‡æ»¤
                db_list.append(text)
        
        # ç®€å•å»é‡
        db_list = list(set(db_list))
        
        cn_dbs = []
        other_dbs = []
        
        for db in db_list:
            if is_chinese(db):
                cn_dbs.append(db)
            else:
                other_dbs.append(db)
                
        return cn_dbs, other_dbs

    except Exception as e:
        return None, None

# --- Streamlit é¡µé¢ UI ---

st.set_page_config(page_title="é«˜æ ¡æ•°æ®åº“ç»Ÿè®¡åŠ©æ‰‹", layout="wide")

st.title("ğŸ“š é«˜æ ¡å›¾ä¹¦é¦†æ•°æ®åº“è‡ªåŠ¨ç»Ÿè®¡å™¨")
st.markdown("è¾“å…¥å­¦æ ¡åç§°ï¼Œç¨‹åºå°†å°è¯•è‡ªåŠ¨å¯»æ‰¾å…¶å›¾ä¹¦é¦†é¡µé¢å¹¶ç»Ÿè®¡æ•°æ®åº“æ•°é‡ã€‚")

school_input = st.text_input("è¯·è¾“å…¥å­¦æ ¡å…¨ç§°ï¼ˆä¾‹å¦‚ï¼šè¥¿å®‰äº¤é€šå¤§å­¦ï¼‰", "")

if st.button("å¼€å§‹åˆ†æ"):
    if not school_input:
        st.warning("è¯·å…ˆè¾“å…¥å­¦æ ¡åç§°")
    else:
        with st.status(f"æ­£åœ¨å¤„ç† {school_input} ...", expanded=True) as status:
            
            # ç¬¬ä¸€æ­¥ï¼šæœç´¢ URL
            status.write("ğŸ” æ­£åœ¨æœç´¢å›¾ä¹¦é¦†æ•°æ®åº“ç½‘å€...")
            target_url = search_library_url(school_input)
            
            if target_url:
                st.success(f"æ‰¾åˆ°ç–‘ä¼¼åœ°å€: {target_url}")
                
                # ç¬¬äºŒæ­¥ï¼šæŠ“å–ä¸åˆ†æ
                status.write("â¬‡ï¸ æ­£åœ¨ä¸‹è½½å¹¶è§£æé¡µé¢å†…å®¹...")
                cn_list, en_list = analyze_page(target_url)
                
                if cn_list is not None:
                    status.update(label="å¤„ç†å®Œæˆ!", state="complete", expanded=False)
                    
                    # --- å±•ç¤ºç»“æœ ---
                    col1, col2, col3 = st.columns(3)
                    col1.metric("ä¸­æ–‡æ•°æ®åº“ (ä¼°ç®—)", f"{len(cn_list)} ä¸ª")
                    col2.metric("å¤–æ–‡/å…¶ä»–æ•°æ®åº“ (ä¼°ç®—)", f"{len(en_list)} ä¸ª")
                    col3.metric("æ€»è®¡", f"{len(cn_list) + len(en_list)} ä¸ª")
                    
                    st.divider()
                    
                    c1, c2 = st.columns(2)
                    with c1:
                        st.subheader("ğŸ“‹ è¯†åˆ«åˆ°çš„ä¸­æ–‡åº“ (éƒ¨åˆ†)")
                        st.dataframe(cn_list, use_container_width=True)
                    with c2:
                        st.subheader("ğŸ“‹ è¯†åˆ«åˆ°çš„å¤–æ–‡åº“ (éƒ¨åˆ†)")
                        st.dataframe(en_list, use_container_width=True)
                        
                else:
                    status.update(label="æŠ“å–å¤±è´¥", state="error")
                    st.error("æ— æ³•è®¿é—®è¯¥ç½‘é¡µï¼Œå¯èƒ½æ˜¯éœ€è¦æ ¡å†…ç½‘(VPN)æ‰èƒ½è®¿é—®ï¼Œæˆ–è€…ç½‘é¡µæœ‰åçˆ¬è™«éªŒè¯ã€‚")
            else:
                status.update(label="æœç´¢å¤±è´¥", state="error")
                st.error("æœªæ‰¾åˆ°è¯¥å­¦æ ¡çš„æ•°æ®åº“åˆ—è¡¨é¡µé¢ï¼Œè¯·æ£€æŸ¥æ ¡åæ˜¯å¦æ­£ç¡®ã€‚")